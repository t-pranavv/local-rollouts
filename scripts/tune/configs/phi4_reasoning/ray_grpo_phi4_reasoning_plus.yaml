data_root: ${oc.env:DATA_ROOT, "/mnt/aifshared"}
output_root: ${oc.env:OUTPUT_ROOT, "~/phyagi_results"}
output_dir: ${output_root}/ray_grpo_phi4_reasoning_plus
tokenizer:
  pretrained_tokenizer_name_or_path: microsoft/phi-4-reasoning-plus

dataset:
  - label: phi_math_new
    train_file: ${data_root}/post_train_data/phi_math_new_formatted/train.parquet
    tokenizer: ${tokenizer.pretrained_tokenizer_name_or_path}
    messages_column_name: prompt
    ground_truth_column_name: ground_truth
    max_length: 1024
    filter_max_length: true

  # Validation datasets
  # `filter_max_length` here is only set to true to reproduce the original results, but almost always
  # it should be set to false to avoid biasing the validation results
  - label: AIME24
    validation_file: ${data_root}/post_train_data/phi_math_new_formatted/AIME24.parquet
    tokenizer: ${tokenizer.pretrained_tokenizer_name_or_path}
    messages_column_name: prompt
    ground_truth_column_name: ground_truth
    max_length: 1024
    filter_max_length: true

  - label: AIME25
    validation_file: ${data_root}/post_train_data/phi_math_new_formatted/AIME25.parquet
    tokenizer: ${tokenizer.pretrained_tokenizer_name_or_path}
    messages_column_name: prompt
    ground_truth_column_name: ground_truth
    max_length: 1024
    filter_max_length: true

  - label: GPQA-diamond-MC
    validation_file: ${data_root}/post_train_data/phi_math_new_formatted/GPQA_diamond_mc.parquet
    tokenizer: ${tokenizer.pretrained_tokenizer_name_or_path}
    messages_column_name: prompt
    ground_truth_column_name: ground_truth
    max_length: 1024
    filter_max_length: true

rewards:
  - name: reward
    type: phi4rp
    kwargs:
      tokenizer: ${tokenizer.pretrained_tokenizer_name_or_path}

tuning_args:
  n_nodes: 1
  n_gpus_per_node: 8
  do_final_eval: true
  eval_before_training: true
  reward_num_workers: 8
  epochs: 1
  log_n_eval_completions: 120
  save_steps: 5
  save_final_checkpoint: true
  checkpoint_mode: async
  eval_steps: 5
  group_size: 8
  eval_group_size: 8
  train_batch_size: 64
  eval_batch_size: null
  train_max_micro_batch_size_per_gpu: 1
  num_policy_updates_per_batch: 1
  kl_coeff: 0.001
  entropy_coeff: 0.001
  dataloader_shuffle: true
  normalize_advantage_std: true
  loss_normalization: sequence
  seed: 1
  actor:
    model:
      pretrained_model_name_or_path: <pretrained_model_name_or_path>
      n_positions: 32768
    optimizer:
      betas: [0.9, 0.999]
      weight_decay: 1.0e-2
    scheduler:
      warmup_num_steps: 23
      warmup_max_lr: 2.0e-07
    fsdp_offload: true
    dtype: bfloat16
    activation_checkpointing: true
    gradient_clipping: 0.2
  rollout:
    disable_log_stats: true # for debugging
    prompt_length: 1024
    response_length: 31744
    tensor_parallel_size: 2
    dtype: bfloat16
    gpu_memory_utilization: 0.5
    enforce_eager: true
    enable_prefix_caching: false
    kv_cache_dtype: fp8
    max_num_batched_tokens: 33792
    preemption_mode: swap
    swap_space: 80
    sampling_params:
      temperature: 1.0
    eval_sampling_params:
      temperature: 0.6
      top_p: 1.0
  wandb:
    key: ${oc.env:WANDB_TOKEN}
    host: ${oc.env:WANDB_HOST}
    project: ${oc.env:WANDB_PROJECT}
