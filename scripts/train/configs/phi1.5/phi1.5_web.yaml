# Phi-1.5 (web) pre-training consists of 2.52 epochs (59.609B x 2.52 = 150.21B tokens)
data_root: ${oc.env:AMLT_DATA_DIR, "/mnt/phyagi"}
output_root: ${oc.env:OUTPUT_ROOT, "~/phyagi_results"}
output_dir: ${output_root}/phi1.5_web
dataset:
  - cache_dir: ${data_root}/datasets/fast_datasets/falcon/falcon_v1_04/shard_0 # 10.155B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/fast_datasets/falcon/falcon_v1_04/shard_1 # 12.288B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/lm_datasets/the-stack-dedup-python-filtered/5 # 5.363B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/lm_datasets/textbook/v4 # 0.742B x 2 = 1.484B
    seq_len: 2048
    weight: 2.0
  - cache_dir: ${data_root}/datasets/lm_datasets/stackoverflow-with-meta-data-filtered/6 # 0.847B x 2 = 1.694B
    seq_len: 2048
    weight: 2.0
  - cache_dir: ${data_root}/datasets/fast_datasets/textbook_mixture_v2/gpt4/train.npy # 0.685B x 3 = 2.055B
    seq_len: 2048
    weight: 3.0
  - cache_dir: ${data_root}/datasets/lm_datasets/omg-08 # 0.179B x 10 = 1.790B
    seq_len: 2048
    weight: 10.0
  - cache_dir: ${data_root}/datasets/fast_datasets/Synthetic/NLP/0803/gsm8k_nlp # 0.195B x 4 = 0.780B
    seq_len: 2048
    weight: 4.0
  - cache_dir: ${data_root}/datasets/fast_datasets/Synthetic/NLP/0803/mixed_v2_purify # 0.214B x 4 = 0.856B
    seq_len: 2048
    weight: 4.0
  - cache_dir: ${data_root}/datasets/fast_datasets/Synthetic/NLP/0803/textbook # 2.036B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/fast_datasets/Synthetic/NLP/0803/textbook_s # 0.715B x 2 = 1.430B
    seq_len: 2048
    weight: 2.0
  - cache_dir: ${data_root}/datasets/fast_datasets/Synthetic/NLP/0818/textbook_m # 1.631B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/fast_datasets/Synthetic/NLP/0818/winostory_merged_v10 # 2.626B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/fast_datasets/Synthetic/NLP/0803/WinoSynthV4 # 1.774B x 2 = 3.548B
    seq_len: 2048
    weight: 2.0
  - cache_dir: ${data_root}/datasets/fast_datasets/Synthetic/NLP/0818/WinoQA_v4 # 0.761B x 3 = 2.283B
    seq_len: 2048
    weight: 3.0
  - cache_dir: ${data_root}/datasets/fast_datasets/Synthetic/NLP/0818/LifeMoments_v3 # 1.316B x 3 = 3.948B
    seq_len: 2048
    weight: 3.0
  - cache_dir: ${data_root}/datasets/fast_datasets/Synthetic/NLP/0829/omg_nlp_v5 # 1.751B x 2 = 3.502B
    seq_len: 2048
    weight: 2.0
  - cache_dir: ${data_root}/datasets/fast_datasets/Synthetic/NLP/0803/gsm8k_code # 0.428B x 5 = 2.140B
    seq_len: 2048
    weight: 5.0
dataset_concat: random
dataset_provider: lm
model:
  model_type: mixformer-sequential
  architecture:
    block_cls: parallel
    mixer:
      mixer_cls: mha
      dropout: 0.1
    mlp:
      mlp_cls: fused_mlp
  vocab_size: 50304
  n_positions: 2048
  n_embd: 2048
  n_layer: 24
  n_head: 32
  rotary_dim: 32
  resid_pdrop: 0.1
  pad_vocab_size_multiple: 64
training_args:
  ds_config:
    fp16:
      enabled: true
      initial_scale_power: 12
    optimizer:
      type: AdamW
      params:
        lr: 1.0e-3
        betas:
        - 0.9
        - 0.95
        eps: 1.0e-7
        weight_decay: 0.1
    scheduler:
      type: WarmupDecayLR
      params:
        warmup_min_lr: 0
        warmup_max_lr: 1.0e-3
        warmup_type: linear
        warmup_num_steps: 750
    zero_optimization:
      stage: 1
    gradient_clipping: 1.0
    steps_per_print: 10000000000
    train_batch_size: 1024
    train_micro_batch_size_per_gpu: 4
    wall_clock_breakdown: false
    zero_allow_untested_optimizer: true
  do_final_eval: true
  num_train_epochs: 2.52
  logging_steps: 0.0001
  save_steps: 0.05
  save_final_checkpoint: true
  seed: 42
  eval_steps: 0.1
  eval_max_steps: 100
  pipe_parallel_size: 1