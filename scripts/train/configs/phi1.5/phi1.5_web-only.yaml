# Phi-1.5 (web-only) pre-training consists of 1.36 epochs (110.803B x 1.36 = 150.69B tokens)
data_root: ${oc.env:AMLT_DATA_DIR, "/mnt/phyagi"}
output_root: ${oc.env:OUTPUT_ROOT, "~/phyagi_results"}
output_dir: ${output_root}/phi1.5_web-only
dataset:
  - cache_dir: ${data_root}/datasets/fast_datasets/falcon/falcon_v1_15/shard_0 # 7.284B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/fast_datasets/falcon/falcon_v1_15/shard_1 # 7.216B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/fast_datasets/falcon/falcon_v1_15/shard_2 # 7.111B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/fast_datasets/falcon/falcon_v1_15/shard_3 # 7.004B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/fast_datasets/falcon/falcon_v1_15/shard_4 # 7.087B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/fast_datasets/falcon/falcon_v1_15/shard_5 # 7.074B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/fast_datasets/falcon/falcon_v1_15/shard_6 # 7.004B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/fast_datasets/falcon/falcon_v1_15/shard_7 # 7.112B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/fast_datasets/falcon/falcon_v1_15/shard_8 # 7.146B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/fast_datasets/falcon/falcon_v1_15/shard_9 # 7.140B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/fast_datasets/falcon/falcon_v1_15/shard_10 # 7.098B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/fast_datasets/falcon/falcon_v1_15/shard_11 # 9.361B
    seq_len: 2048
    weight: 1.0
  - cache_dir: ${data_root}/datasets/Filtering/stack-dedup-python-llm-filtered/codegen_tokenizer/threshold_5.5.npy # 2.908B x 3 = 8.724B
    seq_len: 2048
    weight: 3.0
  - cache_dir: ${data_root}/datasets/fast_datasets/Filtered/stackoverflow/4 # 4.814B x 3 = 14.442B
    seq_len: 2048
    weight: 3.0
dataset_concat: random
dataset_provider: lm
model:
  model_type: mixformer-sequential
  architecture:
    block_cls: parallel
    mixer:
      mixer_cls: mha
      dropout: 0.1
    mlp:
      mlp_cls: fused_mlp
  vocab_size: 50304
  n_positions: 2048
  n_embd: 2048
  n_layer: 24
  n_head: 32
  rotary_dim: 32
  resid_pdrop: 0.1
  pad_vocab_size_multiple: 64
training_args:
  ds_config:
    fp16:
      enabled: true
      initial_scale_power: 12
    optimizer:
      type: AdamW
      params:
        lr: 1.0e-3
        betas:
        - 0.9
        - 0.95
        eps: 1.0e-7
        weight_decay: 0.1
    scheduler:
      type: WarmupDecayLR
      params:
        warmup_min_lr: 0
        warmup_max_lr: 1.0e-3
        warmup_type: linear
        warmup_num_steps: 750
    zero_optimization:
      stage: 1
    gradient_clipping: 1.0
    steps_per_print: 10000000000
    train_batch_size: 1024
    train_micro_batch_size_per_gpu: 4
    wall_clock_breakdown: false
    zero_allow_untested_optimizer: true
  do_final_eval: true
  num_train_epochs: 1.36
  logging_steps: 0.0001
  save_steps: 0.05
  save_final_checkpoint: true
  seed: 42
  eval_steps: 0.1
  eval_max_steps: 100
  pipe_parallel_size: 1