# az ml job create --file autotune.yaml --resource-group ai-frontiers-rg --workspace-name ai-frontiers-sa-ws
$schema: https://azuremlschemas.azureedge.net/latest/sweepJob.schema.json
type: sweep
compute: /subscriptions/5c9e4789-4852-4ffe-8551-d682affcbd74/resourceGroups/ai-frontiers-rg/providers/microsoft.machinelearningservices/virtualclusters/ai-frontiers-sa-vc
resources:
  instance_count: 1
  # instance_type: Singularity.ND12_H100_v5 # 1 GPU
  # instance_type: Singularity.ND24_H100_v5 # 2 GPUs
  # instance_type: Singularity.ND48_H100_v5 # 4 GPUs
  instance_type: Singularity.ND96r_H100_v5 # 8 GPUs
  properties:
    ComputeSpecification:
      Automatic: false
    AISuperComputer:
      imageVersion: ''
      interactive: false
      sshPublicKey: ""
      enableAzmlInt: true
      # priority: Low
      priority: Medium
      # priority: High
      # slaTier: Standard
      slaTier: Premium
experiment_name: model_autotune
tags:
  Project_Name: Orca
  ProjectID: PRJ-0438-A49
  Experiment: Reasoning_LLM
trial:
  code: ../../../../
  distribution:
    type: pytorch
    process_count_per_instance: 1
  command: |
    # Workaround for windows-sa nvidia driver issue
    export LD_LIBRARY_PATH=$(echo $LD_LIBRARY_PATH | tr ':' '\n' | grep -v "compat/lib" | tr '\n' ':' | sed 's/:$//')

    # Print IB topology
    nvidia-smi topo -m
    
    # General Configuration
    export MODEL_NAME=$(echo ${{search_space.model_config}} | cut -d',' -f1)
    export MODEL_PATH=$(echo ${{search_space.model_config}} | cut -d',' -f2)
    export TOKENIZER_PATH=$(echo ${{search_space.model_config}} | cut -d',' -f3)
    export MAX_MODEL_LEN=$(echo ${{search_space.model_config}} | cut -d',' -f4)
    export DTYPE=$(echo ${{search_space.model_config}} | cut -d',' -f5)
    export TENSOR_PARALLEL_SIZE=$(echo ${{search_space.model_config}} | cut -d',' -f6)
    export PIPELINE_PARALLEL_SIZE=$(echo ${{search_space.model_config}} | cut -d',' -f7)

    export SEED=${{search_space.seed}}
    export GENERATION_CONFIG="auto"
    export OVERRIDE_GENERATION_CONFIG='{"temperature": ${{search_space.temperature}}, "skip_special_tokens": false}'

    # Experiment Configuration
    export EXPERIMENT_OUTPUT_DIR=$(echo ${{outputs.output_dir}}/$MODEL_NAME/seed_${SEED}_temperature_${{search_space.temperature}})
    
    # GPU Configuration
    export NODE_RANK=${NODE_RANK:-0}
    export NODES=${AZUREML_NODE_COUNT:-1}
    export GPUS=${GPU_PER_NODE_COUNT:-1}
    # export WORLD_SIZE=$(( NODES * GPUS ))
    export TOTAL_GPUS=$(( NODES * GPUS ))

    export TPP=$(( TENSOR_PARALLEL_SIZE * PIPELINE_PARALLEL_SIZE ))
    export DATA_PARALLEL_SIZE=$(( TOTAL_GPUS / TPP > 0 ? TOTAL_GPUS / TPP : 1 ))
    export DATA_PARALLEL_SIZE_LOCAL=$(( GPUS / TPP > 0 ? GPUS / TPP : 1 ))

    # Server Configuration
    export API_SERVER_COUNT=$DATA_PARALLEL_SIZE
    export PORT=9000

    # Auto-tuning Configuration
    export MAX_INPUT_LEN=$(echo ${{search_space.model_config}} | cut -d',' -f8)
    export MAX_OUTPUT_LEN=$(echo ${{search_space.model_config}} | cut -d',' -f9)
    export NUM_SEQS_LIST="$(echo ${{search_space.model_config}} | cut -d',' -f10)"
    export NUM_BATCHED_TOKENS_LIST="$(echo ${{search_space.model_config}} | cut -d',' -f11)"

    # cd into vllm inference scripts directory
    cd tools/vllm_inference

    # model checkpoint conversion
    if [[ $MODEL_NAME == *-hf ]]; then
      echo "Skip conversion due to -hf suffix indicating HF model..."
      if [[ -d ${{outputs.output_ckpt}}/huggingface_models/$MODEL_PATH ]]; then
        echo "HF model already exists..."
      else
        echo "Downloading HF model from Hugging Face..."
        if [ $NODE_RANK -eq 0 ]; then
          python download_and_save_hf_models.py -m $MODEL_PATH -t $TOKENIZER_PATH -o ${{outputs.output_ckpt}}/huggingface_models
          echo "HF model downloaded and saved to ${{outputs.output_ckpt}}/huggingface_models/$MODEL_PATH"
        else
          echo "Node $NODE_RANK waiting for HF model to be downloaded on rank 0"
          while [ ! -d "${{outputs.output_ckpt}}/huggingface_models/$MODEL_PATH" ]; do
            sleep 5
          done
          echo "HF model downloaded, proceeding on node $NODE_RANK..."
        fi
      fi
      export MODEL_PATH=${{outputs.output_ckpt}}/huggingface_models/$MODEL_PATH
      export TOKENIZER_PATH=$MODEL_PATH
    elif [[ -d ${{outputs.output_ckpt}}/$MODEL_PATH/phi4 ]]; then
      echo "Skip conversion, hf in phi4 class model already exists..."
      export MODEL_PATH=${{outputs.output_ckpt}}/$MODEL_PATH/phi4
      export TOKENIZER_PATH=${{inputs.tokenizer}}/$TOKENIZER_PATH
    else
      echo "Converting model from Zero3 to FP32 and then to HF format..."
      if [ $NODE_RANK -eq 0 ]; then
        echo "Node $NODE_RANK starting conversion..."
        python -m phyagi.cli.interface convert ${{outputs.output_ckpt}}/$MODEL_PATH phi4 -t ${{inputs.tokenizer}}/$TOKENIZER_PATH --dtype=bfloat16 --debug_logit --debug_params --from_deepspeed_zero --save_intermediate_checkpoint # (for saving convertion to fp32)
        echo "Conversion completed on rank 0"
      else
        echo "Node $NODE_RANK waiting for conversion to complete on rank 0..."
        while [ ! -d "${{outputs.output_ckpt}}/$MODEL_PATH/phi4" ]; do
          sleep 5
        done
        echo "Conversion completed, proceeding on node $NODE_RANK..."
      fi
      export MODEL_PATH=${{outputs.output_ckpt}}/$MODEL_PATH/phi4
      export TOKENIZER_PATH=${{inputs.tokenizer}}/$TOKENIZER_PATH
    fi

    echo "Starting VLLM servers with the following configuration:"
    echo "============================ GENERAL CONFIGURATION ============================"
    echo "MODEL_NAME: $MODEL_NAME"
    echo "MODEL_PATH: $MODEL_PATH"
    echo "TOKENIZER_PATH: $TOKENIZER_PATH"
    echo "MAX_MODEL_LEN: $MAX_MODEL_LEN"
    echo "DTYPE: $DTYPE"
    echo "SEED: $SEED"
    echo "GENERATION_CONFIG: $GENERATION_CONFIG"
    echo "OVERRIDE_GENERATION_CONFIG: $OVERRIDE_GENERATION_CONFIG"
    echo "============================ SERVER CONFIGURATION ============================"
    echo "API_SERVER_COUNT: $API_SERVER_COUNT"
    echo "PORT: $PORT"
    echo "TENSOR_PARALLEL_SIZE: $TENSOR_PARALLEL_SIZE"
    echo "PIPELINE_PARALLEL_SIZE: $PIPELINE_PARALLEL_SIZE"
    echo "DATA_PARALLEL_SIZE: $DATA_PARALLEL_SIZE"
    echo "DATA_PARALLEL_SIZE_LOCAL: $DATA_PARALLEL_SIZE_LOCAL"
    echo "============================ GPU CONFIGURATION ============================"
    echo "NODE_RANK: $NODE_RANK"
    echo "NODES: $NODES"
    echo "GPUS: $GPUS"
    echo "WORLD_SIZE: $WORLD_SIZE"    
    echo "TPP: $TPP"
    echo "TOTAL_GPUS: $TOTAL_GPUS"
    echo "MASTER_ADDR: $MASTER_ADDR"
    echo "MASTER_PORT: $MASTER_PORT"
    echo "============================ EXPERIMENT CONFIGURATION ============================"
    echo "EXPERIMENT_OUTPUT_DIR: $EXPERIMENT_OUTPUT_DIR"
    echo "============================ AUTO-TUNING CONFIGURATION ============================"
    echo "MAX_INPUT_LEN: $MAX_INPUT_LEN"
    echo "MAX_OUTPUT_LEN: $MAX_OUTPUT_LEN"
    echo "NUM_SEQS_LIST: $NUM_SEQS_LIST"
    echo "NUM_BATCHED_TOKENS_LIST: $NUM_BATCHED_TOKENS_LIST"
    echo "==============================================================================="
    
    # Run Auto-tuning
    cd modeltune/
    bash autotune.sh
  environment: azureml:reasoning-lm-tooluse:220825
  environment_variables:
    JOB_EXECUTION_MODE: Basic
    AZUREML_COMPUTE_USE_COMMON_RUNTIME: 'true'
    AZUREML_COMMON_RUNTIME_USE_SBOM_CAPABILITY: 'false'
    _AZUREML_SINGULARITY_JOB_UAI: /subscriptions/5c9e4789-4852-4ffe-8551-d682affcbd74/resourcegroups/ai-frontiers-rg/providers/Microsoft.ManagedIdentity/userAssignedIdentities/ai-frontiers-id
inputs:
  tokenizer:
    type: uri_folder
    path: azureml://datastores/aifrontierssadata/paths/phi_ckpts/
    mode: ro_mount
outputs:
  output_ckpt:
    type: uri_folder
    path: azureml://datastores/aifrontierssadata/paths/phi_ckpts/
    mode: rw_mount
  output_dir:
    type: uri_folder
    path: azureml://datastores/aifrontierssadata/paths/autotune/
    mode: rw_mount
sampling_algorithm: grid
search_space:
  model_config:
    type: choice
    values:
      - Phi-4-reasoning-hf,microsoft/Phi-4-reasoning,microsoft/Phi-4-reasoning,32768,bfloat16,1,1,28672,4096,"128 256 512","512 1024 2048 4096"
      # - Qwen2.5-14B-Instruct-hf,Qwen/Qwen2.5-14B-Instruct,Qwen/Qwen2.5-14B-Instruct,32768,bfloat16,1,1,28672,4096,"128 256 512","512 1024 2048 4096"
      # - Qwen2.5-32B-Instruct-hf,Qwen/Qwen2.5-32B-Instruct,Qwen/Qwen2.5-32B-Instruct,32768,bfloat16,1,1,28672,4096,"128 256 512","512 1024 2048 4096"
      # - Qwen3-14B-hf,Qwen/Qwen3-14B,Qwen/Qwen3-14B,32768,bfloat16,1,1,28672,4096,"128 256 512","512 1024 2048 4096"
      # - Qwen3-32B-hf,Qwen/Qwen3-32B,Qwen/Qwen3-32B,32768,bfloat16,1,1,28672,4096,"128 256 512","512 1024 2048 4096"
      # - Qwen3-32B-FP8-hf,Qwen/Qwen3-32B-FP8,Qwen/Qwen3-32B-FP8,32768,bfloat16,1,1,28672,4096,"128 256 512","512 1024 2048 4096"
      # - Qwen3-30B-A3B-hf,Qwen/Qwen3-30B-A3B,Qwen/Qwen3-30B-A3B,32768,bfloat16,1,1,28672,4096,"128 256 512","512 1024 2048 4096"
      # - Qwen3-235B-A22B-FP8-hf,Qwen/Qwen3-235B-A22B-FP8,Qwen/Qwen3-235B-A22B-FP8,32768,bfloat16,4,1,28672,4096,"128 256 512","512 1024 2048 4096"
      # - Qwen3-235B-A22B-hf,Qwen/Qwen3-235B-A22B,Qwen/Qwen3-235B-A22B,32768,bfloat16,8,1,28672,4096,"128 256 512","512 1024 2048 4096"
      - gpt-oss-120b-hf,openai/gpt-oss-120b,openai/gpt-oss-120b,131072,auto,8,1,126976,4096,"128 256 512","512 1024 2048 4096"
      # - gpt-oss-20b-hf,openai/gpt-oss-20b,openai/gpt-oss-20b,131072,auto,1,1,126976,4096,"128 256 512","512 1024 2048 4096"
  temperature:
    type: choice
    values:
      - 0.8
  seed:
    type: choice
    values:
      - 42
objective:
  primary_metric: dummy_metric
  goal: maximize
limits:
  max_total_trials: 1000
  max_concurrent_trials: 4
