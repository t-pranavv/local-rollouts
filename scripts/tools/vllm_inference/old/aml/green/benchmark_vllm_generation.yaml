# az ml job create --file benchmark_vllm_generation.yaml --resource-group msr-aifrontiers-dev-eastus2-rg --workspace-name msr-aifrontiers-dev-eastus2-ws
$schema: https://azuremlschemas.azureedge.net/latest/sweepJob.schema.json
type: sweep
compute: /subscriptions/a0c66465-bfb8-4525-84d0-75a8fb5a054f/resourceGroups/msr-aifrontiers-dev-eastus2-rg/providers/Microsoft.MachineLearningServices/virtualClusters/msr-aif-dev-eastus2-vc
resources:
  instance_count: 1
  # instance_type: Singularity.ND12_H100_v5 # 1 GPU
  # instance_type: Singularity.ND24_H100_v5 # 2 GPUs
  # instance_type: Singularity.ND48_H100_v5 # 4 GPUs
  instance_type: Singularity.ND96r_H100_v5 # 8 GPUs
  properties:
    ComputeSpecification:
      Automatic: false
    AISuperComputer:
      imageVersion: ''
      interactive: false
      sshPublicKey: ""
      enableAzmlInt: true
      # priority: Low
      # priority: Medium
      priority: High
      # slaTier: Standard
      slaTier: Premium
      location: null
      locations: ["eastus2"]
experiment_name: model_evaluation
tags:
  Project_Name: Orca
  ProjectID: PRJ-0438-A49
  Experiment: Reasoning_LLM
trial:
  # code: ./
  code: ../../../../../../
  command: |
    export MODEL_NAME=$(echo ${{search_space.model_config}} | cut -d',' -f1)
    export MODEL_PATH=$(echo ${{search_space.model_config}} | cut -d',' -f2)
    export MAX_SEQ_LEN=$(echo ${{search_space.model_config}} | cut -d',' -f3)
    export TENSOR_PARALLEL_SIZE=$(echo ${{search_space.model_config}} | cut -d',' -f4)
    export TOKENIZER_PATH=$(echo ${{search_space.model_config}} | cut -d',' -f5)
    if [ -z "$TOKENIZER_PATH" ]; then
      export TOKENIZER_PATH=$MODEL_PATH
    fi
    export NUM_SERVERS=$(($VLLM_LOCAL_NUM_INSTANCES / $TENSOR_PARALLEL_SIZE))

    export VLLM_MODEL_NAME=$(echo vllm-local-$MODEL_NAME)

    echo "MODEL_NAME: $MODEL_NAME"
    echo "MODEL_PATH: $MODEL_PATH"
    echo "MAX_SEQ_LEN: $MAX_SEQ_LEN"
    echo "TOKENIZER_PATH: $TOKENIZER_PATH"
    echo "VLLM_MODEL_NAME: $VLLM_MODEL_NAME"
    echo "TENSOR_PARALLEL_SIZE: $TENSOR_PARALLEL_SIZE"
    echo "NUM_SERVERS: $NUM_SERVERS"

    export OPENAI_API_KEY="key"
    export NCCL_DEBUG=WARN
    export PYTHONPATH="$(pwd):$PYTHONPATH"
    cd scripts/tools/vllm_inference/old


    # start vllm servers
    source start_vllm_servers_with_tensor_parallel.sh --world-size=$VLLM_LOCAL_NUM_INSTANCES --pretrained-model=${{inputs.ckpt}}/$MODEL_PATH --tokenizer-model=${{inputs.ckpt}}/$TOKENIZER_PATH --base-port=$VLLM_LOCAL_BASE_PORT --api-key=$OPENAI_API_KEY --max-model-len=$MAX_SEQ_LEN --served-model-name=$VLLM_MODEL_NAME --dtype=bfloat16 --seed=${{search_space.seed}} --generation-config=auto --override-generation-config='{\"temperature\": ${{search_space.temperature}}, \"top_p\": 0.95, \"do_sample\": true, \"use_cache\":true}' --tensor-parallel-size=$TENSOR_PARALLEL_SIZE

    export PORTS=$(seq $VLLM_LOCAL_BASE_PORT $(($VLLM_LOCAL_BASE_PORT + $NUM_SERVERS - 1)))
    echo "PORTS: $PORTS"

    for PORT in $PORTS; do
      echo "Waiting for VLLM server on port $PORT..."
      while ! curl -s http://localhost:$PORT > /dev/null 2>&1; do
        echo "Waiting for VLLM server on port $PORT..."
        sleep 5
      done
      echo "VLLM server ready on port $PORT"
    done

    for PORT in $PORTS; do
      echo "Running benchmark on port $PORT..."
      vllm bench serve --label ${VLLM_MODEL_NAME} --model $VLLM_MODEL_NAME --tokenizer ${{inputs.ckpt}}/$TOKENIZER_PATH --disable-tqdm --port $PORT 
    done

    # vllm serve ${{inputs.ckpt}}/$MODEL_PATH \
    #   --tokenizer ${{inputs.ckpt}}/$TOKENIZER_PATH \
    #   --port $VLLM_LOCAL_BASE_PORT \
    #   --api-key $OPENAI_API_KEY \
    #   --max-model-len $MAX_SEQ_LEN \
    #   --served-model-name $VLLM_MODEL_NAME \
    #   --dtype bfloat16 \
    #   --seed ${{search_space.seed}} \
    #   --max-num-seqs 128 \
    #   --generation-config auto \
    #   --override-generation-config '{"temperature": ${{search_space.temperature}}, "top_p": 0.95, "do_sample": true, "use_cache":true}' \
    #   --tensor-parallel-size $TENSOR_PARALLEL_SIZE \
    #   --data-parallel-size $DATA_PARALLEL_SIZE > /dev/null &

    # echo "Waiting for VLLM servers to be ready..."

    # while ! curl -s http://localhost:$VLLM_LOCAL_BASE_PORT > /dev/null 2>&1; do
    #   echo "Waiting for VLLM server on port $VLLM_LOCAL_BASE_PORT..."
    #   sleep 5
    # done
    # echo "VLLM server ready on port $VLLM_LOCAL_BASE_PORT"
    
    # Run benchmark per GPU
    # echo "Running benchmark..."
    # vllm bench serve --label ${VLLM_MODEL_NAME} --model $VLLM_MODEL_NAME --tokenizer ${{inputs.ckpt}}/$TOKENIZER_PATH --disable-tqdm

  environment: azureml:AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu:10
  environment_variables:
    JOB_EXECUTION_MODE: Basic
    AZUREML_COMPUTE_USE_COMMON_RUNTIME: 'true'
    AML_LOCATION: "eastus2"
    IPP_IMAGE_NAME: amegreenresources.azurecr.io/aif/reasoning-lm-tooluse:110825 
    VLLM_LOCAL_NUM_INSTANCES: '8'
    VLLM_LOCAL_BASE_PORT: '8000'
    VLLM_LOGGING_LEVEL: DEBUG 
inputs:
  ckpt:
    type: uri_folder
    path: azureml://datastores/workspaceblobstore/paths/azureml/
    mode: ro_mount
  tokenizer:
    type: uri_folder
    path: azureml://datastores/workspaceblobstore/paths/azureml/
    mode: ro_mount

sampling_algorithm: grid
search_space:
  model_config:
    type: choice
    values:
      #14B
      - sft_15b_phi4_mathv2code_coco5e_oaif_high32k_10xlr,sft_15b_phi4_mathv2code_coco5e_oaif_high32k_10xlr/output/14025/hf,32768,1,huggingface_models/Phi-4-reasoning
      - sft_16b_phi4_mathv2code5_otcode_oaif_safe_high32_2,sft_16b_phi4_mathv2code5_otcode_oaif_safe_high32_2/output/15325/hf,32768,1,huggingface_models/Phi-4-reasoning
      - Phi-4-reasoning-hf,huggingface_models/Phi-4-reasoning,32768,1
      - Phi-4-reasoning-plus-hf,huggingface_models/Phi-4-reasoning-plus,32768,1
      - phi-4-hf,huggingface_models/phi-4,16384,1,huggingface_models/Phi-4-reasoning
      #7B
      - PhiSilica-hf,tool_use/Phi-7B/reasoning,8192,1
      - sft_7b_mathv2code5_otcode_oaif_safe_low8k_zero3_2,sft_7b_mathv2code5_otcode_oaif_safe_low8k_zero3_2/output/10600/phi4,8192,1
      #3B
      - sft_mini_mathv2code5_otcodeoaif_h32k_lr5e-5_2,sft_mini_mathv2code5_otcodeoaif_h32k_lr5e-5_2/output/14287/hf/,32768,1
      # Qwen 
      - Qwen2.5-14B-Instruct,tool_use/huggingface_models/Qwen2.5-14B-Instruct,32768,1
      - Qwen2.5-32B-Instruct,tool_use/huggingface_models/Qwen2.5-32B-Instruct,32768,1
      - Qwen3-14B,tool_use/huggingface_models/Qwen3-14B,32768,1
      - Qwen3-32B,tool_use/huggingface_models/Qwen3-32B,32768,1
      - Qwen3-32B-FP8,tool_use/huggingface_models/Qwen3-32B-FP8,32768,1
      - Qwen3-30B-A3B,tool_use/huggingface_models/Qwen3-30B-A3B,32768,1
      - Qwen3-235B-A22B-FP8,tool_use/huggingface_models/Qwen3-235B-A22B-FP8,32768,4
      - Qwen3-235B-A22B,tool_use/huggingface_models/Qwen3-235B-A22B,32768,8
      #14B quantized - doesnt work do not use
      # - Qwen2.5-14B-Instruct-1M,tool_use/huggingface_models/Qwen2.5-14B-Instruct-1M,1010000,1
      # - phi-4-reasoning-unsloth-bnb-4bit-hf,huggingface_models/phi-4-reasoning-unsloth-bnb-4bit,32768,1,huggingface_models/Phi-4-reasoning
      # - phi-4-reasoning-IQ4_NL-hf,huggingface_models/Phi-4-reasoning-GGUF/phi-4-reasoning-IQ4_NL.gguf,32768,1,huggingface_models/Phi-4-reasoning
      # - phi-4-reasoning-IQ4_XS-hf,huggingface_models/Phi-4-reasoning-GGUF/phi-4-reasoning-IQ4_XS.gguf,32768,1,huggingface_models/Phi-4-reasoning
      # - phi-4-reasoning-Q4_0-hf,huggingface_models/Phi-4-reasoning-GGUF/phi-4-reasoning-Q4_0.gguf,32768,1,huggingface_models/Phi-4-reasoning
      # - phi-4-reasoning-Q4_1-hf,huggingface_models/Phi-4-reasoning-GGUF/phi-4-reasoning-Q4_1.gguf,32768,1,huggingface_models/Phi-4-reasoning
      # - phi-4-reasoning-Q4_K_M-hf,huggingface_models/Phi-4-reasoning-GGUF/phi-4-reasoning-Q4_K_M.gguf,32768,1,huggingface_models/Phi-4-reasoning
      # - phi-4-reasoning-Q8_0-hf,huggingface_models/Phi-4-reasoning-GGUF/phi-4-reasoning-Q8_0.gguf,32768,1,huggingface_models/Phi-4-reasoning
      # - phi-4-reasoning-UD-Q4_K_XL-hf,huggingface_models/Phi-4-reasoning-GGUF/phi-4-reasoning-UD-Q4_K_XL.gguf,32768,1,huggingface_models/Phi-4-reasoning
      # - phi-4-reasoning-UD-Q8_K_XL-hf,huggingface_models/Phi-4-reasoning-GGUF/phi-4-reasoning-UD-Q8_K_XL.gguf,32768,1,huggingface_models/Phi-4-reasoning
  temperature:
    type: choice
    values:
      - 0.8
  seed:
    type: choice
    values:
      - 42
objective:
  primary_metric: dummy_metric
  goal: maximize
limits:
  max_total_trials: 1000
  max_concurrent_trials: 8
