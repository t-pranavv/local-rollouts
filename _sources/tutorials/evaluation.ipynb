{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "The evaluation module allows models to be tested on tasks they have not been explicitly trained for, thus providing insights into their generalization capabilities and adaptability to novel situations. By implementing a user-friendly evaluation framework in PhyAGI, users can effortlessly gauge their models' performance and determine their suitability for a wide range of applications.\n",
    "\n",
    "## Loading tokenizer and model\n",
    "\n",
    "The first step is to load the tokenizer, which must be the same as the model has been trained on. This is a requirement because it is used to convert the evaluation data to inputs. In this example, we will use the `gpt2` tokenizer from the Hugging Face hub, as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the tokenizer, we can load the model. Again, we will use the `gpt2` model from the Hugging Face hub, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e49730ad1574759991555d81660e672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", device_map=\"cuda\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "PhyAGI provides a **task‑centric** API: you select a task object, pass the model and tokenizer, and call `run()`.   The task takes care of:\n",
    "\n",
    "- Downloading or generating the dataset.\n",
    "- Formatting inputs and expected outputs.\n",
    "- Computing the chosen metric(s).\n",
    "\n",
    "Below we measure accuracy on **PIQA** (physical commonsense reasoning) with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "100%|██████████| 1838/1838 [00:43<00:00, 42.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7872687704026116, 'accuracy_norm': 0.7921653971708379}\n"
     ]
    }
   ],
   "source": [
    "from phyagi.eval.tasks.piqa import PIQA\n",
    "\n",
    "results = PIQA.run(model, tokenizer, batch_size=8)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are dozens of pre-defined tasks, which covers a variety of applications, such as question answering, common-sense reasoning, code generation, among others. The full list of tasks can be found [here](https://microsoft.github.io/phyagi-sdk/api/eval)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrations\n",
    "\n",
    "Prefer a familiar benchmark harness? PhyAGI can act as a thin wrapper around popular suites such as [LM-Eval](https://github.com/EleutherAI/lm-evaluation-harness), letting you keep a single workflow while tapping into extra tasks.\n",
    "\n",
    "## LM-Eval\n",
    "\n",
    "Before using LM-Eval, you need to install it by running the following command (root folder of PhyAGI):\n",
    "\n",
    "```bash\n",
    "pip install -e .[eval]\n",
    "```\n",
    "\n",
    "Once installed, the code cell below shows how to launch an LM‑Eval run that internally reuses the same model object loaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lm_eval.models.huggingface:`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "WARNING:lm_eval.models.huggingface:Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "WARNING:lm_eval.evaluator:Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1538.31it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 3676/3676 [00:19<00:00, 188.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'piqa': {'accuracy': 0.7856365614798694, 'accuracy_stderr': 0.009574842136050943, 'accuracy_norm': 0.7927094668117519, 'accuracy_norm_stderr': 0.009457844699952379}}\n"
     ]
    }
   ],
   "source": [
    "from phyagi.eval.tasks.lm_evaluation_harness import LMEvaluationHarness\n",
    "\n",
    "results = LMEvaluationHarness.run(model, tokenizer, tasks=\"piqa\", batch_size=8)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customization\n",
    "\n",
    "In this section, we will show how to customize the evaluation module to fit your needs. The only requirement for implementing a custom evaluation task is to implement a `run` function, which takes the `model` and `tokenizer` as positional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "from phyagi.eval.generation import example_generator\n",
    "from phyagi.eval.log_likelihood_pipeline import LogLikelihoodPipeline\n",
    "from phyagi.utils.file_utils import save_json_file\n",
    "\n",
    "\n",
    "class ARCEasy:\n",
    "    @staticmethod\n",
    "    def mapping_fn(example: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        prompt = \"Question: {}\\nAnswer:{}\"\n",
    "        targets = [\" \" + choice for choice in example[\"choices\"][\"text\"]]\n",
    "\n",
    "        answer_key_map = {\"1\": \"A\", \"2\": \"B\", \"3\": \"C\", \"4\": \"D\", \"5\": \"E\"}\n",
    "        answer_key = answer_key_map.get(example[\"answerKey\"], example[\"answerKey\"])\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                \"source\": prompt.format(example[\"question\"], target),\n",
    "                \"target\": target,\n",
    "                \"label\": [\"A\", \"B\", \"C\", \"D\", \"E\"].index(answer_key),\n",
    "            }\n",
    "            for target in targets\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def run(model: torch.nn.Module, tokenizer: PreTrainedTokenizerBase, output_file_path: Optional[str] = None, **kwargs) -> Dict[str, Any]:\n",
    "        pipeline = LogLikelihoodPipeline(model, tokenizer)\n",
    "        dataset = load_dataset(\"ai2_arc\", name=\"ARC-Easy\")[\"test\"]\n",
    "        metric = {\n",
    "            \"accuracy\": load(\"accuracy\"),\n",
    "            \"accuracy_norm\": load(\"accuracy\"),\n",
    "        }\n",
    "        outputs = []\n",
    "\n",
    "        for output in tqdm(\n",
    "            pipeline(example_generator(dataset, mapping_fn=ARCEasy.mapping_fn), **kwargs), total=len(dataset)\n",
    "        ):\n",
    "            log_likelihoods = output[\"log_likelihoods\"]\n",
    "            target_lengths = output[\"target_lengths\"]\n",
    "            label = output[\"label\"]\n",
    "\n",
    "            prediction = np.argmax(log_likelihoods)\n",
    "            prediction_norm = np.argmax(np.array(log_likelihoods) / target_lengths)\n",
    "\n",
    "            metric[\"accuracy\"].add(predictions=prediction, reference=label)\n",
    "            metric[\"accuracy_norm\"].add(predictions=prediction_norm, reference=label)\n",
    "\n",
    "            outputs.append(output)\n",
    "\n",
    "        save_json_file(outputs, output_file_path) if output_file_path else None\n",
    "\n",
    "        return {key: metric.compute()[\"accuracy\"] for key, metric in metric.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core computation of the task is defined by the `LogLikelihoodPipeline`, which is an extension of the `transformer.Pipeline` class and provides a straighforward way to compute the log-likelihood of a given sentence. Additionally, since it relies on Hugging Face API, we do not need to worry about batched inference or multi-GPU usage, since the pipeline implements these features under the hood.\n",
    "\n",
    "Even though using pipelines are not required, PhyAGI provides a nice set of pipelines that makes it easier to implement custom evaluation tasks. Please take a look over the [documentation](https://microsoft.github.io/phyagi-sdk/api/eval.html#utilities) to check the pre-defined pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phyagisdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
