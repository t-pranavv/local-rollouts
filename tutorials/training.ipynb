{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This tutorial provides a step-by-step guide to training a model on a dataset.\n",
    "\n",
    "There are three steps involved in training a model:\n",
    "\n",
    "1. Tokenizing and loading the data.\n",
    "\n",
    "2. Creating the model\n",
    "\n",
    "3. Training the model.\n",
    "\n",
    "We will explore each of these steps in detail.\n",
    "\n",
    "## Tokenizing and loading the data\n",
    "\n",
    "The initial step involves either tokenizing a new dataset or loading a previously tokenized one. We will employ the `LMDatasetProvider` class and utilize a dataset from the [Hugging Face datasets library](https://huggingface.co/datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[phyagi] [2025-05-19 10:45:15,798] [INFO] [lm_dataset_provider.py:252:from_hub] Loading non-tokenized dataset...\n",
      "[phyagi] [2025-05-19 10:45:17,999] [WARNING] [lm_dataset_provider.py:270:from_hub] 'cache' already exists and will be overritten.\n",
      "[phyagi] [2025-05-19 10:45:18,000] [INFO] [lm_dataset_provider.py:285:from_hub] Creating validation split (if necessary)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cc88484a1341bfa5aee0590bedba36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset with shared memory...:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f09986a96a46ea907c3ee7f8029ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset with shared memory...:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6602a5e52de746b49c31df0342dd06a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset with shared memory...:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[phyagi] [2025-05-19 10:45:19,442] [INFO] [lm_dataset_provider.py:309:from_hub] Saving tokenized dataset: cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from phyagi.datasets.train.lm.lm_dataset_provider import LMDatasetProvider\n",
    "\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "dataset_provider = LMDatasetProvider.from_hub(\n",
    "    dataset_path=\"wikitext\",\n",
    "    dataset_name=\"wikitext-2-raw-v1\",\n",
    "    tokenizer=\"gpt2\",\n",
    "    cache_dir=\"cache\",\n",
    "    seq_len=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been tokenized, a set of NumPy arrays will be generated in the `cache` directory. If the data has already been tokenized, you can load it from the cache directory using the following code: `dataset_provider = LMDatasetProvider.from_cache(\"cache\")`.\n",
    "\n",
    "These arrays represent continuous blocks of tokenized data that can be swiftly loaded into memory and divided into different sequence lengths. With the dataset provider prepared, you can obtain the training and validation datasets by calling the `get_train_dataset` and `get_val_dataset` methods, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset_provider.get_train_dataset()\n",
    "val_dataset = dataset_provider.get_val_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "After loading the datasets, you can create a model using a configuration object, which is similar to the Hugging Face API. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "config = GPT2Config(n_layer=4, n_embd=192)\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example employs the default configuration (apart from the number of layers and embeddings) arguments to create the model. However, you can easily modify these by altering the arguments passed to `GPT2Config`.  \n",
    "   \n",
    "## Training the model  \n",
    "   \n",
    "With the data prepared and the model loaded, we can now proceed to train the model. PhyAGI offers two training classes: `DsTrainer` and `HfTrainer`. The former utilizes DeepSpeed for training the model, while the latter relies on Hugging Face's `transformers.Trainer` class.\n",
    "\n",
    "### DeepSpeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[phyagi] [2025-05-19 10:45:20,526] [WARNING] [ds_trainer.py:321:_prepare_ds_config] `scheduler.params.total_num_steps` not provided. Setting to 1 steps.\n",
      "[2025-05-19 10:45:20,528] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.7, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-19 10:45:20,529] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 1\n",
      "[2025-05-19 10:45:20,588] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-05-19 10:45:20,589] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2025-05-19 10:45:20,590] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-05-19 10:45:20,591] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2025-05-19 10:45:20,592] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale\n",
      "[2025-05-19 10:45:20,597] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = FP16_Optimizer\n",
      "[2025-05-19 10:45:20,598] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started\n",
      "[2025-05-19 10:45:20,599] [WARNING] [lr_schedules.py:759:__init__] total_num_steps 1 is less than warmup_num_steps 1000\n",
      "[2025-05-19 10:45:20,600] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupDecayLR\n",
      "[2025-05-19 10:45:20,600] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupDecayLR object at 0x7fd255f45900>\n",
      "[2025-05-19 10:45:20,601] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[[0.9, 0.95]]\n",
      "[2025-05-19 10:45:20,602] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:\n",
      "[2025-05-19 10:45:20,603] [INFO] [config.py:1007:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-05-19 10:45:20,603] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-05-19 10:45:20,604] [INFO] [config.py:1007:print]   amp_enabled .................. False\n",
      "[2025-05-19 10:45:20,605] [INFO] [config.py:1007:print]   amp_params ................... False\n",
      "[2025-05-19 10:45:20,606] [INFO] [config.py:1007:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-05-19 10:45:20,606] [INFO] [config.py:1007:print]   bfloat16_enabled ............. False\n",
      "[2025-05-19 10:45:20,607] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-05-19 10:45:20,608] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-05-19 10:45:20,608] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-05-19 10:45:20,609] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-05-19 10:45:20,610] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd31c631f60>\n",
      "[2025-05-19 10:45:20,610] [INFO] [config.py:1007:print]   communication_data_type ...... None\n",
      "[2025-05-19 10:45:20,611] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False\n",
      "[2025-05-19 10:45:20,612] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-05-19 10:45:20,612] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False\n",
      "[2025-05-19 10:45:20,613] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False\n",
      "[2025-05-19 10:45:20,614] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-05-19 10:45:20,614] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False\n",
      "[2025-05-19 10:45:20,615] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False\n",
      "[2025-05-19 10:45:20,616] [INFO] [config.py:1007:print]   disable_allgather ............ False\n",
      "[2025-05-19 10:45:20,616] [INFO] [config.py:1007:print]   dump_state ................... False\n",
      "[2025-05-19 10:45:20,617] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
      "[2025-05-19 10:45:20,617] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False\n",
      "[2025-05-19 10:45:20,618] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-05-19 10:45:20,619] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-05-19 10:45:20,619] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-05-19 10:45:20,620] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-05-19 10:45:20,626] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-05-19 10:45:20,627] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-05-19 10:45:20,627] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False\n",
      "[2025-05-19 10:45:20,628] [INFO] [config.py:1007:print]   elasticity_enabled ........... False\n",
      "[2025-05-19 10:45:20,629] [INFO] [config.py:1007:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-05-19 10:45:20,629] [INFO] [config.py:1007:print]   fp16_auto_cast ............... False\n",
      "[2025-05-19 10:45:20,630] [INFO] [config.py:1007:print]   fp16_enabled ................. True\n",
      "[2025-05-19 10:45:20,631] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-05-19 10:45:20,631] [INFO] [config.py:1007:print]   global_rank .................. 0\n",
      "[2025-05-19 10:45:20,632] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None\n",
      "[2025-05-19 10:45:20,633] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 256\n",
      "[2025-05-19 10:45:20,633] [INFO] [config.py:1007:print]   gradient_clipping ............ 0.0\n",
      "[2025-05-19 10:45:20,634] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-05-19 10:45:20,635] [INFO] [config.py:1007:print]   graph_harvesting ............. False\n",
      "[2025-05-19 10:45:20,635] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-05-19 10:45:20,636] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 4096\n",
      "[2025-05-19 10:45:20,637] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False\n",
      "[2025-05-19 10:45:20,637] [INFO] [config.py:1007:print]   loss_scale ................... 0\n",
      "[2025-05-19 10:45:20,638] [INFO] [config.py:1007:print]   memory_breakdown ............. False\n",
      "[2025-05-19 10:45:20,639] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False\n",
      "[2025-05-19 10:45:20,639] [INFO] [config.py:1007:print]   mics_shard_size .............. -1\n",
      "[2025-05-19 10:45:20,640] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-05-19 10:45:20,641] [INFO] [config.py:1007:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-05-19 10:45:20,642] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-05-19 10:45:20,642] [INFO] [config.py:1007:print]   optimizer_name ............... adamw\n",
      "[2025-05-19 10:45:20,643] [INFO] [config.py:1007:print]   optimizer_params ............. {'lr': 0.0018, 'betas': [0.9, 0.95], 'eps': 1e-07}\n",
      "[2025-05-19 10:45:20,644] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-05-19 10:45:20,644] [INFO] [config.py:1007:print]   pld_enabled .................. False\n",
      "[2025-05-19 10:45:20,645] [INFO] [config.py:1007:print]   pld_params ................... False\n",
      "[2025-05-19 10:45:20,646] [INFO] [config.py:1007:print]   prescale_gradients ........... False\n",
      "[2025-05-19 10:45:20,646] [INFO] [config.py:1007:print]   scheduler_name ............... WarmupDecayLR\n",
      "[2025-05-19 10:45:20,647] [INFO] [config.py:1007:print]   scheduler_params ............. {'warmup_min_lr': 0.0, 'warmup_max_lr': 0.0018, 'warmup_type': 'linear', 'total_num_steps': 1}\n",
      "[2025-05-19 10:45:20,648] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-05-19 10:45:20,648] [INFO] [config.py:1007:print]   sparse_attention ............. None\n",
      "[2025-05-19 10:45:20,649] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False\n",
      "[2025-05-19 10:45:20,650] [INFO] [config.py:1007:print]   steps_per_print .............. 1000\n",
      "[2025-05-19 10:45:20,650] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-05-19 10:45:20,651] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-05-19 10:45:20,652] [INFO] [config.py:1007:print]   train_batch_size ............. 1024\n",
      "[2025-05-19 10:45:20,653] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  4\n",
      "[2025-05-19 10:45:20,653] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False\n",
      "[2025-05-19 10:45:20,654] [INFO] [config.py:1007:print]   use_node_local_storage ....... False\n",
      "[2025-05-19 10:45:20,655] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False\n",
      "[2025-05-19 10:45:20,655] [INFO] [config.py:1007:print]   weight_quantization_config ... None\n",
      "[2025-05-19 10:45:20,656] [INFO] [config.py:1007:print]   world_size ................... 1\n",
      "[2025-05-19 10:45:20,657] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  False\n",
      "[2025-05-19 10:45:20,657] [INFO] [config.py:1007:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-05-19 10:45:20,658] [INFO] [config.py:1007:print]   zero_enabled ................. False\n",
      "[2025-05-19 10:45:20,659] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-05-19 10:45:20,659] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 0\n",
      "[2025-05-19 10:45:20,660] [INFO] [config.py:993:print_user_config]   json = {\n",
      "    \"train_batch_size\": 1.024000e+03, \n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"initial_scale_power\": 12\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 0\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.0018, \n",
      "            \"betas\": [0.9, 0.95], \n",
      "            \"eps\": 1e-07\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupDecayLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0.0, \n",
      "            \"warmup_max_lr\": 0.0018, \n",
      "            \"warmup_type\": \"linear\", \n",
      "            \"total_num_steps\": 1\n",
      "        }\n",
      "    }, \n",
      "    \"steps_per_print\": 1000\n",
      "}\n",
      "[phyagi] [2025-05-19 10:45:20,667] [INFO] [ds_trainer.py:236:__init__] Model: GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 192)\n",
      "    (wpe): Embedding(1024, 192)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x GPT2Block(\n",
      "        (ln_1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=576, nx=192)\n",
      "          (c_proj): Conv1D(nf=192, nx=192)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=768, nx=192)\n",
      "          (c_proj): Conv1D(nf=192, nx=768)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=192, out_features=50257, bias=False)\n",
      ")\n",
      "[phyagi] [2025-05-19 10:45:20,682] [INFO] [ds_trainer.py:237:__init__] Training arguments: {'output_dir': '/home/gderosa/phyagi-sdk/docs/tutorials/gpt2-wt103', 'ds_config': {'train_batch_size': 1024, 'train_micro_batch_size_per_gpu': 4, 'fp16': {'enabled': True, 'initial_scale_power': 12}, 'zero_optimization': {'stage': 0}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 0.0018, 'betas': [0.9, 0.95], 'eps': 1e-07}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 0.0, 'warmup_max_lr': 0.0018, 'warmup_type': 'linear', 'total_num_steps': 1}}, 'steps_per_print': 1000}, 'do_eval': True, 'do_final_eval': False, 'train_batch_size_init_rampup': 0, 'train_batch_size_per_rampup': 0, 'rampup_steps': 0, 'num_train_epochs': 1, 'max_steps': 1, 'logging_steps': 10, 'save_steps': 500, 'save_final_checkpoint': False, 'eval_steps': 500, 'eval_max_steps': None, 'seed': 42, 'pipe_parallel_size': 1, 'pipe_parallel_partition_method': 'parameters', 'pipe_parallel_activation_checkpoint_steps': 0, 'tensor_parallel_size': 1, 'context_parallel_size': 1, 'batch_tracker': False, 'batch_tracker_save_steps': 10, 'dataloader_shuffle': True, 'eval_dataloader_shuffle': True, 'dataloader_pin_memory': True, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'load_checkpoint_num_tries': 1, 'backend': None, 'timeout': 1800, 'log_dir': '/home/gderosa/phyagi-sdk/docs/tutorials/gpt2-wt103', 'mlflow': False, 'wandb': False, 'wandb_api_key': None, 'wandb_host': None, 'tensorboard': False}\n",
      "[phyagi] [2025-05-19 10:45:20,684] [INFO] [ds_trainer.py:656:train] Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[phyagi] [2025-05-19 10:45:28,362] [INFO] [ds_trainer.py:761:train] {'step/step_runtime': 7.674737930297852, 'step/samples_per_second': 133.42475134655956, 'step/samples_per_second_per_gpu': 133.42475134655956, 'step/tokens_per_second': 133.42475134655956, 'step/tokens_per_second_per_gpu': 133.42475134655956, 'step/tflops': 0.007211353769554979, 'step/tflops_per_gpu': 0.007211353769554979, 'step/mfu': 2.3284965352131026e-05, 'train/total_runtime': 7.676496267318726, 'train/progress': 1.0, 'train/epoch': 0.5, 'train/step': 1, 'train/loss': 10.843445777893066, 'train/loss_scale': 4096, 'train/gradient_norm': 2.3035886052062624, 'train/ppl': 51197.48956505967, 'train/learning_rate': 0.0, 'train/batch_size': 1024, 'train/n_samples': 1024, 'train/n_tokens': 1024, 'train/step_runtime': 7.674936771392822, 'train/samples_per_second': 133.42129459838768, 'train/samples_per_second_per_gpu': 133.42129459838768, 'train/tokens_per_second': 133.42129459838768, 'train/tokens_per_second_per_gpu': 133.42129459838768, 'train/tflops': 0.007211166938897937, 'train/tflops_per_gpu': 0.007211166938897937, 'train/mfu': 2.3284362088788948e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[phyagi] [2025-05-19 10:45:28,365] [INFO] [ds_trainer.py:790:train] Training done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from phyagi.trainers.ds.ds_trainer import DsTrainer\n",
    "from phyagi.trainers.ds.ds_training_args import DsTrainingArguments\n",
    "\n",
    "training_args = DsTrainingArguments(\"gpt2-wt103\", max_steps=1)\n",
    "trainer = DsTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepSpeed uses `deepspeed` launcher. For instance, to train the model with 4 GPUs, you can execute the following command:  \n",
    "   \n",
    "```bash  \n",
    "deepspeed --num_gpus=4 train.py  \n",
    "```  \n",
    "   \n",
    "Make sure to replace `train.py` with the appropriate Python script containing your training code.\n",
    "\n",
    "### Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=10.859561920166016, metrics={'train_runtime': 0.5342, 'train_samples_per_second': 14.976, 'train_steps_per_second': 1.872, 'total_flos': 87482695680.0, 'train_loss': 10.859561920166016, 'epoch': 0.003389830508474576})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from phyagi.trainers.hf.hf_trainer import HfTrainer\n",
    "\n",
    "# Re-initialize the model to avoid the training to continue from the previous run\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "training_args = TrainingArguments(\"gpt2-wt103\", max_steps=1)\n",
    "trainer = HfTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face requires the `torch` launcher to be used for multi-GPU training. For instance, to train the model with 4 GPUs, you can execute the following command:  \n",
    "   \n",
    "```bash  \n",
    "torchrun --nproc-per-node=4 train.py  \n",
    "```  \n",
    "   \n",
    "Make sure to replace `train.py` with the appropriate Python script containing your training code.\n",
    "\n",
    "### PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [rank: 0] Seed set to 42\n",
      "INFO:lightning.fabric.utilities.seed:[rank: 0] Seed set to 42\n",
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: `Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "INFO: `Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "INFO: `Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "INFO: `Trainer(limit_predict_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer(limit_predict_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "/home/gderosa/miniconda3/envs/phyagisdk/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory /home/gderosa/phyagi-sdk/docs/tutorials/gpt2-wt103/ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/home/gderosa/miniconda3/envs/phyagisdk/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/gderosa/phyagi-sdk/docs/tutorials/gpt2-wt103 exists and is not empty.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name  | Type            | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 11.6 M | train\n",
      "--------------------------------------------------\n",
      "11.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.6 M    Total params\n",
      "46.503    Total estimated model params size (MB)\n",
      "60        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name  | Type            | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 11.6 M | train\n",
      "--------------------------------------------------\n",
      "11.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.6 M    Total params\n",
      "46.503    Total estimated model params size (MB)\n",
      "60        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f80586477d40e6bbe211294af43e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gderosa/miniconda3/envs/phyagisdk/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/gderosa/miniconda3/envs/phyagisdk/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n",
      "/home/gderosa/miniconda3/envs/phyagisdk/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e649e61780404338a5d8574464674d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_steps=1` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    }
   ],
   "source": [
    "from phyagi.trainers.pl.pl_trainer import PlTrainer\n",
    "from phyagi.trainers.pl.pl_training_args import PlTrainingArguments, PlTrainerArguments, PlLightningModuleArguments\n",
    "\n",
    "# Re-initialize the model to avoid the training to continue from the previous run\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "training_args = PlTrainingArguments(\"gpt2-wt103\", trainer=PlTrainerArguments(max_steps=1), lightning_module=PlLightningModuleArguments(optimizer={\"type\": \"adamw\"}))\n",
    "trainer = PlTrainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Lightnign does not require any launcher to be used for multi-GPU training. For instance, to train the model with 4 GPUs, you can execute the following command:  \n",
    "   \n",
    "```bash  \n",
    "python train.py  \n",
    "```  \n",
    "   \n",
    "Make sure to replace `train.py` with the appropriate Python script containing your training code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing the training\n",
    "\n",
    "PhyAGI is designed to be flexible and customizable, and training-related components are no exception. It also provides a `DsTrainer` class designed to be used with [DeepSpeed](https://www.deepspeed.ai/). In this tutorial, we will show you how to customize the training process with the `DsTrainer` class.\n",
    "\n",
    "Please note that the approach depicted below is extensible to any trainer in PhyAGI, such as `HfTrainer` and `DDLTrainer`.\n",
    "\n",
    "## Overriding the DeepSpeed trainer\n",
    "\n",
    "As mentioned before, this tutorial will guide you in overriding the `DsTrainer` class. PhyAGI attempts to keep a unified structure between its trainers, following the style provided by the Hugging Face API. This means that the `DsTrainer` class has methods, such as `save_checkpoint`, `load_checkpoint`, `train_step` and `evaluate_step`.\n",
    "\n",
    "In this example, we will override the `train_step` function of the trainer and create a customized training loop. The `train_step` function is called by the `train` function, which is the main training loop of the trainer. The `train_step` function is called once per batch, and it is responsible for performing the forward and backward passes of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, Iterable, Iterator, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LRScheduler\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "\n",
    "from phyagi.trainers.ds.ds_trainer import DsTrainer\n",
    "from phyagi.trainers.ds.ds_trainer_callback import DsTrainerCallback\n",
    "from phyagi.trainers.ds.ds_training_args import DsTrainingArguments\n",
    "\n",
    "\n",
    "class MyDsTrainer(DsTrainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        args: Optional[DsTrainingArguments] = None,\n",
    "        data_collator: Optional[Callable] = None,\n",
    "        sampler: Optional[Sampler] = None,\n",
    "        train_dataset: Optional[Dataset] = None,\n",
    "        eval_dataset: Optional[Dataset] = None,\n",
    "        model_parameters: Optional[Union[Iterable[torch.Tensor], Dict[str, torch.Tensor]]] = None,\n",
    "        mpu: Optional[Any] = None,\n",
    "        dist_init_required: bool = None,\n",
    "        dist_timeout: int = 1800,\n",
    "        callbacks: Optional[List[DsTrainerCallback]] = None,\n",
    "        optimizers: Optional[Tuple[Optimizer, LRScheduler]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            model,\n",
    "            args=args,\n",
    "            data_collator=data_collator,\n",
    "            sampler=sampler,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            model_parameters=model_parameters,\n",
    "            mpu=mpu,\n",
    "            dist_init_required=dist_init_required,\n",
    "            dist_timeout=dist_timeout,\n",
    "            callbacks=callbacks,\n",
    "            optimizers=optimizers,\n",
    "        )\n",
    "\n",
    "    def train_step(self, train_iterator: Iterator) -> torch.Tensor:\n",
    "        gradient_accumulation_steps = self.engine.gradient_accumulation_steps()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for _ in range(gradient_accumulation_steps):\n",
    "            input_ids, labels = next(train_iterator)\n",
    "\n",
    "            input_ids = input_ids.to(self.engine.device)\n",
    "            labels = labels.to(self.engine.device)\n",
    "\n",
    "            outputs = self.engine(input_ids, labels=labels)\n",
    "            loss = outputs[0].mean()\n",
    "\n",
    "            self.engine.backward(loss)\n",
    "            self.engine.step()\n",
    "\n",
    "            total_loss += loss\n",
    "\n",
    "        return total_loss / gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usage of the trainer follows the same procedure depicted by the training model section. The only difference is that we will use the `MyDsTrainer` class instead of the `DsTrainer` class. As mentioned before, every method from the `DsTrainer` class can be overriden and customized according to your needs. This is only a simple example that illustrates how to override the `train_step` function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phyagisdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
