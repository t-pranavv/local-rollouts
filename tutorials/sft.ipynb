{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT)\n",
    "\n",
    "This tutorial provides a comprehensive, step-by-step guide to fine-tuning a model using **Supervised Fine-Tuning (SFT)**. The explanations and examples are based on the following scripts: \n",
    "\n",
    "- [ds_train.py](https://github.com/microsoft/phyagi-sdk/blob/main/scripts/train/ds_train.py).\n",
    "- [hf_sft_tune.py](https://github.com/microsoft/phyagi-sdk/blob/main/scripts/tune/hf_sft_tune.py) \n",
    "\n",
    "Although these scripts are designed for different purposes, i.e., former is for training and latter is for fine-tuning, both can fine-tune a model with SFT as long as the dataset is prepared correctly.\n",
    "\n",
    "## Structure of the tutorial\n",
    "\n",
    "1. **Dataset preparation**: Learn how datasets are pre-defined and processed for fine-tuning.\n",
    "2. **Fine-tuning workflow**: Understand the key steps and scripts used to perform fine-tuning.\n",
    "   \n",
    "**Note**: The code snippets provided in this guide highlight essential sections of the scripts for better understanding. For the complete implementation, refer to the linked scripts above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "Fine-tuning requires carefully curated datasets, often formatted as **JSONL** files. This format is particularly suitable for preference-based tasks, where each record includes:\n",
    "\n",
    "- `chosen`: The preferred response(s) for a given prompt.\n",
    "- `rejected`: The less preferred response(s) for the same prompt.\n",
    "\n",
    "Below is an example JSONL structure illustrating this format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"chosen\": [\n",
    "        {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Please write the steps for founding a student-run venture capital fund\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Starting a student-run venture capital (VC) fund can be an exciting and educational experience...\"}\n",
    "    ],\n",
    "    \"rejected\": [\n",
    "        {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Please write the steps for founding a student-run venture capital fund\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"1. Feasibility Study: Start with a comprehensive feasibility study that includes studying similar organizations, successful models, potential funders, legal requirements, and the operational scope of your venture capital fund...\"}\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation for SFT\n",
    "\n",
    "The goal is to train a model using explicitly labeled datasets to align its outputs with human expectations for specific inputs. The dataset processing workflow for SFT involves the following steps:\n",
    "\n",
    "1. **Extract prompts and completions**  \n",
    "   - Identify the input context (`prompt`) and the desired output (`completion`).  \n",
    "   - For preference-based datasets, the `chosen` responses are treated as the `completion`.  \n",
    "   - Any `rejected` responses are discarded, ensuring that only positive examples are used for training.\n",
    "\n",
    "2. **Apply chat template**  \n",
    "   - A conversational formatting style (e.g., `chatml`) is applied to both the `prompt` and `completion` fields.  \n",
    "   - This step standardizes the dataset into a consistent chat-style format, essential for fine-tuning conversational models.\n",
    "\n",
    "3. **Merge fields**  \n",
    "   - The `prompt` and `completion` fields are concatenated into a single `text` field.  \n",
    "   - This merged structure simplifies tokenization and ensures the dataset is ready for efficient training.\n",
    "   \n",
    "This approach ensures that the dataset is optimized for supervised fine-tuning, enabling the model to learn from high-quality, labeled examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from trl import extract_prompt\n",
    "\n",
    "from phyagi.datasets.rl.formatting_utils import apply_chat_template\n",
    "\n",
    "def prepare_dataset_sft(dataset: Dataset) -> Dataset:\n",
    "    assert isinstance(dataset, Dataset), \"`dataset` must be an instance of datasets.Dataset.\"\n",
    "\n",
    "    # If the dataset has already been prepared, return it\n",
    "    if \"text\" in dataset.column_names:\n",
    "        return dataset\n",
    "\n",
    "    # If the dataset has an implicit prompt, extract it\n",
    "    if \"prompt\" not in dataset.column_names:\n",
    "        dataset = dataset.map(extract_prompt)\n",
    "\n",
    "    # If the dataset is a preference-based dataset, keep only the \"chosen\" examples\n",
    "    # Rename \"chosen\" to \"completion\"\n",
    "    if \"chosen\" in dataset.column_names:\n",
    "        dataset = dataset.remove_columns(\"rejected\").rename_column(\"chosen\", \"completion\")\n",
    "\n",
    "    # Ensure necessary columns exist\n",
    "    assert \"prompt\" in dataset.column_names, \"`prompt` must be available in the dataset.\"\n",
    "    assert \"completion\" in dataset.column_names, \"`completion` must be available in the dataset.\"\n",
    "\n",
    "    # Apply chat template and combine prompt/completion into a single field\n",
    "    dataset = dataset.map(\n",
    "        apply_chat_template, fn_kwargs={\"special_token_format\": \"chatml\", \"shuffle\": False, \"add_mask_tokens\": False}\n",
    "    )\n",
    "    dataset = dataset.map(lambda x: {\"text\": x[\"prompt\"] + x[\"completion\"]}).remove_columns([\"prompt\", \"completion\"])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script overview\n",
    "\n",
    "This section provides high-level overviews of the script structure and key components involved in fine-tuning a model.\n",
    "\n",
    "### DeepSpeed\n",
    "\n",
    "The script starts by parsing the configuration file and any command-line arguments. This step ensures that all necessary configuration fields are properly set, including:\n",
    "\n",
    "- `output_dir`: The directory where the model outputs will be saved.\n",
    "- `dataset`: The dataset configuration, including its location and structure.\n",
    "- `model`: The model configuration, including details on the pre-trained model to be fine-tuned.\n",
    "- `training_args`: The parameters governing the training/fine-tuning process, such as learning rate, batch size, etc.\n",
    "\n",
    "By validating the configuration file, the script ensures that all the required components are available for successful fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from phyagi.utils.config import load_config\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"config_file_path\",\n",
    "        type=str,\n",
    "        nargs=\"*\",\n",
    "        help=\"Path to the YAML configuration file.\",\n",
    "    )\n",
    "\n",
    "    args, extra_args = parser.parse_known_args()\n",
    "\n",
    "    return args, extra_args\n",
    "\n",
    "\n",
    "args, extra_args = parse_args()\n",
    "args.config_file_path = \"ds_sft.yaml\"\n",
    "\n",
    "config = load_config(args.config_file_path, extra_args)\n",
    "\n",
    "assert \"output_dir\" in config, \"`output_dir` must be available in configuration.\"\n",
    "assert \"dataset\" in config, \"`dataset` must be available in configuration.\"\n",
    "assert \"model\" in config, \"`model` must be available in configuration.\"\n",
    "assert \"training_args\" in config, \"`training_args` must be available in configuration.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model and tokenizer are loaded according to the configuration specified in the setup. If a tokenizer is not explicitly provided in the configuration, the script defaults to using the tokenizer associated with the pre-trained model. This ensures that both the model and tokenizer are correctly initialized and compatible, enabling smooth data processing during the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[phyagi] [2025-05-26 13:18:43,703] [INFO] [model.py:93:get_model] Loading pre-trained model: microsoft/phi-1\n",
      "[phyagi] [2025-05-26 13:18:43,705] [INFO] [model.py:94:get_model] Model configuration: {'torch_dtype': torch.float16, 'trust_remote_code': True}\n",
      "[phyagi] [2025-05-26 13:18:44,119] [INFO] [tokenizer.py:60:get_tokenizer] Loading pre-trained tokenizer: microsoft/phi-1\n",
      "[phyagi] [2025-05-26 13:18:44,120] [INFO] [tokenizer.py:61:get_tokenizer] Tokenizer configuration: {'pad_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "from phyagi.models.registry import get_model\n",
    "from phyagi.models.registry import get_tokenizer\n",
    "\n",
    "model = get_model(**config[\"model\"])\n",
    "tokenizer_config = config.get(\"tokenizer\", {})\n",
    "if tokenizer_config.get(\"pretrained_tokenizer_name_or_path\", None) is None:\n",
    "    tokenizer_config[\"pretrained_tokenizer_name_or_path\"] = model.config.name_or_path\n",
    "tokenizer = get_tokenizer(**tokenizer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training arguments are extracted from the configuration file to define key settings such as output directories and training frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phyagi.trainers.registry import get_training_args\n",
    "\n",
    "training_args = get_training_args(config[\"output_dir\"], framework=\"ds\", **config[\"training_args\"])\n",
    "args.checkpoint_dir = training_args.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_trainer` function is responsible for initializing the training environment. It sets up the model, tokenizer, and any optional configurations, ensuring the model is ready for the fine-tuning process with the specified arguments and dataset.\n",
    "\n",
    "**Note 1:** `train_dataset` and `eval_dataset` are loaded using the `load_dataset` function, which reads the dataset configuration and prepares the data for training and evaluation. This has been omitted for brevity and covered in the previous section on dataset preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from phyagi.trainers.registry import get_trainer\n",
    "\n",
    "train_dataset = load_dataset(**config[\"dataset\"], split=\"train\")\n",
    "train_dataset = prepare_dataset_sft(train_dataset)\n",
    "\n",
    "trainer = get_trainer(\n",
    "    model,\n",
    "    framework=\"ds\",\n",
    "    training_args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning process is executed by calling the `trainer.train` method. If a checkpoint directory is specified, fine-tuning will resume from the most recent checkpoint, allowing for continuation of the fine-tuning process without starting over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "trainer.train(\n",
    "    resume_from_checkpoint=args.checkpoint_dir if re.match(r\"checkpoint-\\d+\", args.checkpoint_dir) else False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face\n",
    "\n",
    "The script starts by parsing the configuration file and any command-line arguments. This step ensures that all necessary configuration fields are properly set, including:\n",
    "\n",
    "- `output_dir`: The directory where the model outputs will be saved.\n",
    "- `dataset`: The dataset configuration, including its location and structure.\n",
    "- `model`: The model configuration, including details on the pre-trained model to be fine-tuned.\n",
    "- `tuning_args`: The parameters governing the fine-tuning process, such as learning rate, batch size, etc.\n",
    "\n",
    "By validating the configuration file, the script ensures that all the required components are available for successful fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from phyagi.utils.config import load_config\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"config_file_path\",\n",
    "        type=str,\n",
    "        nargs=\"*\",\n",
    "        help=\"Path to the YAML configuration file.\",\n",
    "    )\n",
    "\n",
    "    args, extra_args = parser.parse_known_args()\n",
    "\n",
    "    return args, extra_args\n",
    "\n",
    "\n",
    "args, extra_args = parse_args()\n",
    "args.config_file_path = \"hf_sft.yaml\"\n",
    "\n",
    "config = load_config(args.config_file_path, extra_args)\n",
    "\n",
    "assert \"output_dir\" in config, \"`output_dir` must be available in configuration.\"\n",
    "assert \"dataset\" in config, \"`dataset` must be available in configuration.\"\n",
    "assert \"model\" in config, \"`model` must be available in configuration.\"\n",
    "assert \"tuning_args\" in config, \"`tuning_args` must be available in configuration.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model and tokenizer are loaded according to the configuration specified in the setup. If a tokenizer is not explicitly provided in the configuration, the script defaults to using the tokenizer associated with the pre-trained model. This ensures that both the model and tokenizer are correctly initialized and compatible, enabling smooth data processing during the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[phyagi] [2025-05-26 13:18:52,489] [INFO] [model.py:93:get_model] Loading pre-trained model: microsoft/phi-1\n",
      "[phyagi] [2025-05-26 13:18:52,492] [INFO] [model.py:94:get_model] Model configuration: {'torch_dtype': torch.float16, 'trust_remote_code': True}\n",
      "[phyagi] [2025-05-26 13:18:52,884] [INFO] [tokenizer.py:60:get_tokenizer] Loading pre-trained tokenizer: microsoft/phi-1\n",
      "[phyagi] [2025-05-26 13:18:52,885] [INFO] [tokenizer.py:61:get_tokenizer] Tokenizer configuration: {'pad_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "from phyagi.models.registry import get_model\n",
    "from phyagi.models.registry import get_tokenizer\n",
    "\n",
    "model = get_model(**config[\"model\"])\n",
    "tokenizer_config = config.get(\"tokenizer\", {})\n",
    "if tokenizer_config.get(\"pretrained_tokenizer_name_or_path\", None) is None:\n",
    "    tokenizer_config[\"pretrained_tokenizer_name_or_path\"] = model.config.name_or_path\n",
    "tokenizer = get_tokenizer(**tokenizer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuning arguments are extracted from the configuration file to define key settings such as output directories, training frameworks, and task-specific parameters. It is important to note that the task type (e.g., SFT or DPO) is specified when retrieving the tuning arguments, allowing for task-specific configurations during the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 13:18:55 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-26 13:18:55 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from phyagi.rl.tuners.registry import get_tuning_args\n",
    "\n",
    "tuning_args = get_tuning_args(config[\"output_dir\"], framework=\"hf\", task=\"sft\", **config[\"tuning_args\"])\n",
    "args.checkpoint_dir = tuning_args.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_tuner` function is responsible for initializing the tuning environment. It sets up the model, tokenizer, and any optional Parameter-Efficient Fine-Tuning (PEFT) configurations, ensuring the model is ready for the fine-tuning process with the specified arguments and dataset.\n",
    "\n",
    "**Note 1:** `get_tuner` function is task-agnostic and can be used for both SFT and DPO fine-tuning, providing a unified interface for initializing the fine-tuning environment.\n",
    "\n",
    "**Note 2:** `train_dataset` and `eval_dataset` are loaded using the `load_dataset` function, which reads the dataset configuration and prepares the data for training and evaluation. This has been omitted for brevity and covered in the previous section on dataset preparation.\n",
    "\n",
    "**Note 3:** `peft_config` is optional and can be used to enable PEFT, a technique that optimizes the fine-tuning process by adjusting the learning rate schedule and other hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from phyagi.rl.tuners.registry import get_tuner\n",
    "\n",
    "train_dataset = load_dataset(**config[\"dataset\"], split=\"train\")\n",
    "train_dataset = prepare_dataset_sft(train_dataset)\n",
    "\n",
    "tuner = get_tuner(\n",
    "    model,\n",
    "    framework=\"hf\",\n",
    "    task=\"sft\",\n",
    "    tuning_args=tuning_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process is executed by calling the `tuner.train` method. If a checkpoint directory is specified, training will resume from the most recent checkpoint, allowing for continuation of the fine-tuning process without starting over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "tuner.train(\n",
    "    resume_from_checkpoint=args.checkpoint_dir if re.match(r\"checkpoint-\\d+\", args.checkpoint_dir) else False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phyagisdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
