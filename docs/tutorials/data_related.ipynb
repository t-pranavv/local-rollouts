{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data‑related\n",
    "\n",
    "This tutorial provides a brief introduction to the **data‑related datasets** in PhyAGI and shows you **how to load** pre‑processed data with `phyagi-sdk`.\n",
    "\n",
    "After completing this tutorial, you will be able to:\n",
    "\n",
    "- Understand the data‑handling features of **PhyAGI**.\n",
    "- Tokenize a dataset from scratch (class‑style *and* functional‑style).\n",
    "- Load pre‑processed data from configuration objects or YAML files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "In PhyAGI (and PyTorch) a **dataset** is a *class* that implements the `__len__` and `__getitem__` methods. Each call to `__getitem__` should return a **dictionary** representing one sample.\n",
    "\n",
    "A dataset is responsible for:\n",
    "\n",
    "- Loading the raw data.\n",
    "- Applying processing or augmentation.\n",
    "- Returning each example as a dictionary with the keys.\n",
    "  - `input_ids`: sequence of input tokens.\n",
    "  - `labels`: sequence of target tokens.\n",
    "\n",
    "*(Optional)* You can attach other fields (e.g. `attention_mask`) directly in the dataset class or add them later via a **data collator**.\n",
    "\n",
    "The following example builds an empty `LMDataset`. Each call to `dataset[i]` returns the dictionary described above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from phyagi.datasets.train.lm.lm_dataset import LMDataset\n",
    "\n",
    "inputs = np.zeros(2048)\n",
    "dataset = LMDataset(inputs, seq_len=128)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset providers\n",
    "\n",
    "When you work with large corpora or multiple files, it is convenient to put all data‑loading logic inside a **dataset provider**. A provider knows how to:\n",
    "\n",
    "1. Locate the raw data (local path, S3 bucket, or Hugging Face Hub).\n",
    "2. Load and cache it.\n",
    "3. Preprocess and tokenize it.\n",
    "4. Return ready‑to‑use *training* and *validation* datasets.\n",
    "\n",
    "The snippet below downloads a dataset from the Hugging Face Hub **once**, preprocesses it, and then reuses the same provider instance to obtain both splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[phyagi] [2025-05-19 09:05:03,655] [INFO] [lm_dataset_provider.py:252:from_hub] Loading non-tokenized dataset...\n",
      "[phyagi] [2025-05-19 09:05:08,092] [WARNING] [lm_dataset_provider.py:270:from_hub] 'cache' already exists and will be overritten.\n",
      "[phyagi] [2025-05-19 09:05:08,094] [INFO] [lm_dataset_provider.py:285:from_hub] Creating validation split (if necessary)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e15d6008964751bab971501a207d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset with shared memory...:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04769294c9b84c2f941aa3cfecf8ae60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset with shared memory...:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7659c398e049e4bf525502352ab921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset with shared memory...:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[phyagi] [2025-05-19 09:05:09,982] [INFO] [lm_dataset_provider.py:309:from_hub] Saving tokenized dataset: cache\n",
      "{'input_ids': tensor([  796,   569, 18354,  ...,   373,  3194, 16385]), 'labels': tensor([  796,   569, 18354,  ...,   373,  3194, 16385])} {'input_ids': tensor([  796,  8074, 20272,  ...,  7423,  1267,   837]), 'labels': tensor([  796,  8074, 20272,  ...,  7423,  1267,   837])}\n"
     ]
    }
   ],
   "source": [
    "from phyagi.datasets.train.lm.lm_dataset_provider import LMDatasetProvider\n",
    "\n",
    "dataset_provider = LMDatasetProvider.from_hub(\n",
    "    dataset_path=\"wikitext\",\n",
    "    dataset_name=\"wikitext-2-raw-v1\",\n",
    "    tokenizer=\"gpt2\",\n",
    ")\n",
    "\n",
    "train_dataset = dataset_provider.get_train_dataset()\n",
    "val_dataset = dataset_provider.get_val_dataset()\n",
    "print(train_dataset[0], val_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collators\n",
    "\n",
    "Just before data enters the model you often need to perform *batch‑level* adjustments such as padding or masking. In PhyAGI you do this with a **data collator**.\n",
    "\n",
    "The example below constructs an `LMDataCollator` that ignores the token with `token_id = 796`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  796,   569, 18354,  ...,   373,  3194, 16385],\n",
      "        [11308,   575, 10546,  ..., 13460,  5670,   319]]), 'labels': tensor([[ -100,   569, 18354,  ...,   373,  3194, 16385],\n",
      "        [11308,   575, 10546,  ..., 13460,  5670,   319]])}\n"
     ]
    }
   ],
   "source": [
    "from phyagi.datasets.train.train_data_collator import LMDataCollator\n",
    "\n",
    "data_collator = LMDataCollator(ignore_token_ids=[796], ignore_index=-100)\n",
    "samples = [train_dataset[0], train_dataset[1]]\n",
    "\n",
    "collated_samples = data_collator(samples)\n",
    "assert collated_samples[\"labels\"][0][0] == -100\n",
    "print(collated_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: `796` is used here only as an example, you can instruct the collator to ignore any token ID, or none at all.*\n",
    "\n",
    "For more details about datasets, providers, and collators, consult the [official documentation](https://microsoft.github.io/phyagi/api/datasets.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing data\n",
    "\n",
    "PhyAGI offers **two approaches** to tokenize a dataset:\n",
    "\n",
    "1. **Class‑style** – wrap the entire workflow in an `LMDatasetProvider`. *Recommended for most users.*  \n",
    "2. **Functional‑style** – call low level helper functions directly. *Useful when you need complete control over preprocessing.*\n",
    "\n",
    "The sections below illustrate both approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class-style\n",
    "\n",
    "`LMDatasetProvider` encapsulates the entire tokenization pipeline. Under the hood it leverages the Hugging Face `datasets` library, caches the processed data, and exposes a clean API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[phyagi] [2025-05-19 09:05:10,858] [INFO] [lm_dataset_provider.py:252:from_hub] Loading non-tokenized dataset...\n",
      "[phyagi] [2025-05-19 09:05:15,888] [WARNING] [lm_dataset_provider.py:270:from_hub] 'cache' already exists and will be overritten.\n",
      "[phyagi] [2025-05-19 09:05:15,890] [INFO] [lm_dataset_provider.py:285:from_hub] Creating validation split (if necessary)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc70787c2be4c758b99bedf40fb939d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset with shared memory...:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b342db2bf424d00a1ec09f5a688cee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset with shared memory...:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93f32ce880742fd92d0a63e9122ae4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset with shared memory...:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[phyagi] [2025-05-19 09:05:18,372] [INFO] [lm_dataset_provider.py:309:from_hub] Saving tokenized dataset: cache\n"
     ]
    }
   ],
   "source": [
    "from phyagi.datasets.train.lm.lm_dataset_provider import LMDatasetProvider\n",
    "\n",
    "dataset_provider = LMDatasetProvider.from_hub(\n",
    "    dataset_path=\"wikitext\",\n",
    "    dataset_name=\"wikitext-2-raw-v1\",\n",
    "    tokenizer=\"gpt2\",\n",
    "    cache_dir=\"cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous snippet tokenizes the entire Wikipedia dataset and stores the result in the `cache/` directory:\n",
    "\n",
    "- `train.npy`: Tokenized training split.\n",
    "- `validation.npy`: Tokenized validation split (if available). \n",
    "- `tokenizer.pkl`: Serialized tokenizer object.\n",
    "\n",
    "You can customize the provider, for example, to change the column name (default is `text`), adjust block size, or supply a different tokenizer by passing the corresponding arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[phyagi] [2025-05-19 09:05:18,925] [INFO] [lm_dataset_provider.py:252:from_hub] Loading non-tokenized dataset...\n",
      "[phyagi] [2025-05-19 09:05:20,718] [WARNING] [lm_dataset_provider.py:270:from_hub] 'cache' already exists and will be overritten.\n",
      "[phyagi] [2025-05-19 09:05:20,719] [INFO] [lm_dataset_provider.py:285:from_hub] Creating validation split (if necessary)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69506bf5eddd429c9aa4ec81a0886f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset with shared memory...:   0%|          | 0/68 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1efceff8824a258da085648f76f77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset with shared memory...:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4930523b465549a7b6e3c0a83071e182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset with shared memory...:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[phyagi] [2025-05-19 09:05:21,556] [INFO] [lm_dataset_provider.py:309:from_hub] Saving tokenized dataset: cache\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "dataset_provider = LMDatasetProvider.from_hub(\n",
    "    dataset_path=\"glue\",\n",
    "    dataset_name=\"sst2\",\n",
    "    tokenizer=tokenizer,\n",
    "    mapping_column_name=\"sentence\",\n",
    "    cache_dir=\"cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional-style\n",
    "\n",
    "If you prefer *not* to use a provider, you can call the low level tokenization utilities directly. The snippet below shows how to tokenize a dataset using plain functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78005f536a3c4c03a13a8d7f9c5109c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309198e621d948bfb8372273826a0f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e455a0d41c084aae8d1d607488715acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "from phyagi.datasets.train.lm.lm_dataset_provider import _tokenize_concatenated\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "mapping_column_name = [\"text\"]\n",
    "dtype = np.uint16 if tokenizer.vocab_size < 64 * 1024 else np.int32\n",
    "cache_dir = \"cache\"\n",
    "\n",
    "dataset = dataset.map(\n",
    "        _tokenize_concatenated,\n",
    "        fn_kwargs={\n",
    "            \"tokenizer\": tokenizer,\n",
    "            \"mapping_column_name\": mapping_column_name,\n",
    "            \"use_eos_token\": True,\n",
    "            \"dtype\": dtype,\n",
    "        },\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "    )\n",
    "dataset.save_to_disk(cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, the functional helpers still rely on the Hugging Face `datasets` library, so the tokenized dataset is saved as an **Arrow** file.\n",
    "\n",
    "If you need a NumPy array you can:\n",
    "\n",
    "1. Load the Arrow dataset into memory with `process_dataset_to_memory`.  \n",
    "2. Save it with `save_numpy_dataset`.\n",
    "\n",
    "Both helpers live in `phyagi.datasets.shared_memory_utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eef254dde9c42e0a0c0c36fba46ede5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset with shared memory...:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d390845ccee474da344390d45e41a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset with shared memory...:   0%|          | 0/37 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489fee81b53747e5a8e988fffae97648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dataset with shared memory...:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_file': PosixPath('cache/test.npy'),\n",
       " 'train_file': PosixPath('cache/train.npy'),\n",
       " 'validation_file': PosixPath('cache/validation.npy'),\n",
       " 'tokenizer_file': PosixPath('cache/tokenizer.pkl')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phyagi.datasets.shared_memory_utils import process_dataset_to_memory, save_memory_dataset\n",
    "\n",
    "processed_dataset_dict = process_dataset_to_memory(\n",
    "    dataset,\n",
    "    cache_dir,\n",
    "    dtype,\n",
    "    [\"input_ids\"],\n",
    "    num_workers=1,\n",
    "    use_shared_memory=True,\n",
    ")\n",
    "save_memory_dataset(processed_dataset_dict, tokenizer, cache_dir, use_shared_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "\n",
    "PhyAGI ships with a collection of **providers** and their respective **configuration classes**. These classes allow you to load pre‑tokenized datasets with a single call without need to repeat the preprocessing steps.\n",
    "\n",
    "### From a configuration object\n",
    "\n",
    "The example below uses the default `LMProviderConfig` to load the dataset tokenized in the previous section.Every provider has its own config class, e.g., the parameters of `LMProviderConfig` can be customised as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  796,   569, 18354,  7496, 17740,  6711,   796,   220,   198, 50256,\n",
      "         2311,    73, 13090,   645,   569, 18354,  7496,   513,  1058,   791,\n",
      "        47398, 17740,   357,  4960,  1058, 10545,   230,    99,   161,   254,\n",
      "          112,  5641, 44444,  9202, 25084, 24440, 12675, 11839,    18,   837,\n",
      "         6578,   764,   569, 18354,  7496,   286,   262, 30193,   513,  1267,\n",
      "          837,  8811,  6412,   284,   355,   569, 18354,  7496, 17740,  6711,\n",
      "         2354,  2869,   837,   318,   257, 16106,  2597,  2488,    12,    31,\n",
      "         2712,  2008,   983,  4166,   416, 29490,   290,  6343,    13, 44206,\n",
      "          329,   262, 14047, 44685,   764, 28728,   287,  3269,  2813,   287,\n",
      "         2869,   837,   340,   318,   262,  2368,   983,   287,   262,   569,\n",
      "        18354,  7496,  2168,   764, 12645,   278,   262,   976, 21748,   286,\n",
      "        16106,   290,  1103,  2488,    12,    31,   640, 11327,   355,   663,\n",
      "        27677,   837,   262,  1621,  4539, 10730,   284,   262,   717,   983,\n",
      "          290,  5679,   262,   366, 17871,  5321,   366,   837,   257, 23634,\n",
      "         2422,  4326,  7351,   262,  3277,   286,  7096,   544,  1141,   262,\n",
      "         5498,  1898,  6839,  1810,   508,  1620,  3200,  2042,  4560,   290,\n",
      "          389, 46852,  1028,   262, 11773,  4326,   366,  2199,   321,   265,\n",
      "           88, 12552,   366,   764,   220,   198, 50256,   383,   983,  2540,\n",
      "         2478,   287,  3050,   837,  6872,   625,   257,  1588,  6903,   286,\n",
      "          262,   670,  1760,   319,   569, 18354,  7496, 17740,  2873,   764,\n",
      "         2893,   340, 17383,   262,  3210,  3033,   286,   262,  2168,   837,\n",
      "          340,   635, 25289,  3294, 16895,   837,   884,   355,  1642,   262,\n",
      "          983,   517, 43486,   329,  2168, 29661,   764, 15684, 11915,   371,\n",
      "         4548,    64,  8835,    73,   280,   290, 26777,  7286, 13704, 13231,\n",
      "        43354,  1111,  4504,   422,  2180, 12784,   837,  1863,   351,   569,\n",
      "        18354,  7496, 17740,  2873,  3437, 33687,  5303, 18024,  6909,   764,\n",
      "          317,  1588,  1074,   286,  8786, 12118,   262,  4226,   764,   383,\n",
      "          983,   705,    82,  4756,  7505,   373, 23568,   416,  1737,   705,\n",
      "           77,   764,   220,   198, 50256,   632,  1138,   351,  3967,  4200,\n",
      "          287,  2869,   837,   290,   373, 15342,   416,  1111,  4960,   290,\n",
      "         8830,  9188,   764,  2293,  2650,   837,   340,  2722, 41496,  2695,\n",
      "          837,  1863,   351,   281,  9902,  8313,   287,  3389,   286,   326,\n",
      "          614,   764,   632,   373,   635, 16573,   656, 15911,   290,   281,\n",
      "         2656,  2008, 11034,  2168,   764, 14444,   284,  1877,  4200,   286,\n",
      "          569, 18354,  7496, 17740,  2873,   837,   569, 18354,  7496, 17740,\n",
      "         6711,   373,   407, 36618,   837,   475,   257,  4336, 11059, 11670,\n",
      "          351,   262,   983,   705,    82,  9902,  8313,   373,  2716,   287,\n",
      "         1946,   764,  6343,    13, 44206,   561,  1441,   284,   262,  8663,\n",
      "          351,   262,  2478,   286,   569, 18354,  7496,  1058, 22134,  9303,\n",
      "          329,   262, 14047,   604,   764,   220,   198, 50256,   796,   796,\n",
      "         3776,  1759,   796,   796,   220,   198, 50256,  1081,   351,  2180,\n",
      "          569, 18354,  8704, 17740,  1830,   837,   569, 18354,  7496, 17740,\n",
      "         6711,   318,   257, 16106,  2597,  2488,    12,    31,  2712,   983,\n",
      "          810,  1938,  1011,  1630,   286,   257,  2422,  4326,   290,  1011,\n",
      "          636,   287, 10566,  1028,  4472,  3386,   764, 18152,   389,  1297,\n",
      "          832,  9048,  1492,  2488,    12,    31,   588, 13043,   351, 15108,\n",
      "         2095, 31725,   837,   351,  3435,  5486, 12387,   832, 21346,  4046,\n",
      "        25037,   290, 12387,   832,   555, 13038,  3711,  2420,   764,   383,\n",
      "         2137, 33226,   832,   257,  2168,   286, 14174, 10566,   837, 11835,\n",
      "        14838,   355,  8739,   326,   460,   307, 12748, 28660,   832,   290,\n",
      "          302, 21542,   355,   484,   389, 14838,   764,   383,  6339,   284,\n",
      "         1123,  1621]), 'labels': tensor([  796,   569, 18354,  7496, 17740,  6711,   796,   220,   198, 50256,\n",
      "         2311,    73, 13090,   645,   569, 18354,  7496,   513,  1058,   791,\n",
      "        47398, 17740,   357,  4960,  1058, 10545,   230,    99,   161,   254,\n",
      "          112,  5641, 44444,  9202, 25084, 24440, 12675, 11839,    18,   837,\n",
      "         6578,   764,   569, 18354,  7496,   286,   262, 30193,   513,  1267,\n",
      "          837,  8811,  6412,   284,   355,   569, 18354,  7496, 17740,  6711,\n",
      "         2354,  2869,   837,   318,   257, 16106,  2597,  2488,    12,    31,\n",
      "         2712,  2008,   983,  4166,   416, 29490,   290,  6343,    13, 44206,\n",
      "          329,   262, 14047, 44685,   764, 28728,   287,  3269,  2813,   287,\n",
      "         2869,   837,   340,   318,   262,  2368,   983,   287,   262,   569,\n",
      "        18354,  7496,  2168,   764, 12645,   278,   262,   976, 21748,   286,\n",
      "        16106,   290,  1103,  2488,    12,    31,   640, 11327,   355,   663,\n",
      "        27677,   837,   262,  1621,  4539, 10730,   284,   262,   717,   983,\n",
      "          290,  5679,   262,   366, 17871,  5321,   366,   837,   257, 23634,\n",
      "         2422,  4326,  7351,   262,  3277,   286,  7096,   544,  1141,   262,\n",
      "         5498,  1898,  6839,  1810,   508,  1620,  3200,  2042,  4560,   290,\n",
      "          389, 46852,  1028,   262, 11773,  4326,   366,  2199,   321,   265,\n",
      "           88, 12552,   366,   764,   220,   198, 50256,   383,   983,  2540,\n",
      "         2478,   287,  3050,   837,  6872,   625,   257,  1588,  6903,   286,\n",
      "          262,   670,  1760,   319,   569, 18354,  7496, 17740,  2873,   764,\n",
      "         2893,   340, 17383,   262,  3210,  3033,   286,   262,  2168,   837,\n",
      "          340,   635, 25289,  3294, 16895,   837,   884,   355,  1642,   262,\n",
      "          983,   517, 43486,   329,  2168, 29661,   764, 15684, 11915,   371,\n",
      "         4548,    64,  8835,    73,   280,   290, 26777,  7286, 13704, 13231,\n",
      "        43354,  1111,  4504,   422,  2180, 12784,   837,  1863,   351,   569,\n",
      "        18354,  7496, 17740,  2873,  3437, 33687,  5303, 18024,  6909,   764,\n",
      "          317,  1588,  1074,   286,  8786, 12118,   262,  4226,   764,   383,\n",
      "          983,   705,    82,  4756,  7505,   373, 23568,   416,  1737,   705,\n",
      "           77,   764,   220,   198, 50256,   632,  1138,   351,  3967,  4200,\n",
      "          287,  2869,   837,   290,   373, 15342,   416,  1111,  4960,   290,\n",
      "         8830,  9188,   764,  2293,  2650,   837,   340,  2722, 41496,  2695,\n",
      "          837,  1863,   351,   281,  9902,  8313,   287,  3389,   286,   326,\n",
      "          614,   764,   632,   373,   635, 16573,   656, 15911,   290,   281,\n",
      "         2656,  2008, 11034,  2168,   764, 14444,   284,  1877,  4200,   286,\n",
      "          569, 18354,  7496, 17740,  2873,   837,   569, 18354,  7496, 17740,\n",
      "         6711,   373,   407, 36618,   837,   475,   257,  4336, 11059, 11670,\n",
      "          351,   262,   983,   705,    82,  9902,  8313,   373,  2716,   287,\n",
      "         1946,   764,  6343,    13, 44206,   561,  1441,   284,   262,  8663,\n",
      "          351,   262,  2478,   286,   569, 18354,  7496,  1058, 22134,  9303,\n",
      "          329,   262, 14047,   604,   764,   220,   198, 50256,   796,   796,\n",
      "         3776,  1759,   796,   796,   220,   198, 50256,  1081,   351,  2180,\n",
      "          569, 18354,  8704, 17740,  1830,   837,   569, 18354,  7496, 17740,\n",
      "         6711,   318,   257, 16106,  2597,  2488,    12,    31,  2712,   983,\n",
      "          810,  1938,  1011,  1630,   286,   257,  2422,  4326,   290,  1011,\n",
      "          636,   287, 10566,  1028,  4472,  3386,   764, 18152,   389,  1297,\n",
      "          832,  9048,  1492,  2488,    12,    31,   588, 13043,   351, 15108,\n",
      "         2095, 31725,   837,   351,  3435,  5486, 12387,   832, 21346,  4046,\n",
      "        25037,   290, 12387,   832,   555, 13038,  3711,  2420,   764,   383,\n",
      "         2137, 33226,   832,   257,  2168,   286, 14174, 10566,   837, 11835,\n",
      "        14838,   355,  8739,   326,   460,   307, 12748, 28660,   832,   290,\n",
      "          302, 21542,   355,   484,   389, 14838,   764,   383,  6339,   284,\n",
      "         1123,  1621])}\n",
      "4718\n"
     ]
    }
   ],
   "source": [
    "from phyagi.datasets.dataset_provider import DatasetProviderConfig\n",
    "from phyagi.datasets.train.lm.lm_dataset_provider import LMDatasetProvider\n",
    "\n",
    "dataset_config = DatasetProviderConfig(cache_dir=\"cache\", seq_len=512)\n",
    "dataset_provider = LMDatasetProvider.from_config(dataset_config)\n",
    "\n",
    "dataset = dataset_provider.get_train_dataset()\n",
    "print(dataset[0])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From a configuration file\n",
    "\n",
    "You can also store the configuration as YAML and reload it later. The snippet below shows how to load an `LMProviderConfig` from `config.yaml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  796,   569, 18354,  7496, 17740,  6711,   796,   220,   198, 50256,\n",
      "         2311,    73, 13090,   645,   569, 18354,  7496,   513,  1058,   791,\n",
      "        47398, 17740,   357,  4960,  1058, 10545,   230,    99,   161,   254,\n",
      "          112,  5641, 44444,  9202, 25084, 24440, 12675, 11839,    18,   837,\n",
      "         6578,   764,   569, 18354,  7496,   286,   262, 30193,   513,  1267,\n",
      "          837,  8811,  6412,   284,   355,   569, 18354,  7496, 17740,  6711,\n",
      "         2354,  2869,   837,   318,   257, 16106,  2597,  2488,    12,    31,\n",
      "         2712,  2008,   983,  4166,   416, 29490,   290,  6343,    13, 44206,\n",
      "          329,   262, 14047, 44685,   764, 28728,   287,  3269,  2813,   287,\n",
      "         2869,   837,   340,   318,   262,  2368,   983,   287,   262,   569,\n",
      "        18354,  7496,  2168,   764, 12645,   278,   262,   976, 21748,   286,\n",
      "        16106,   290,  1103,  2488,    12,    31,   640, 11327,   355,   663,\n",
      "        27677,   837,   262,  1621,  4539, 10730,   284,   262,   717,   983,\n",
      "          290,  5679,   262,   366, 17871,  5321,   366,   837,   257, 23634,\n",
      "         2422,  4326,  7351,   262,  3277,   286,  7096,   544,  1141,   262,\n",
      "         5498,  1898,  6839,  1810,   508,  1620,  3200,  2042,  4560,   290,\n",
      "          389, 46852,  1028,   262, 11773,  4326,   366,  2199,   321,   265,\n",
      "           88, 12552,   366,   764,   220,   198, 50256,   383,   983,  2540,\n",
      "         2478,   287,  3050,   837,  6872,   625,   257,  1588,  6903,   286,\n",
      "          262,   670,  1760,   319,   569, 18354,  7496, 17740,  2873,   764,\n",
      "         2893,   340, 17383,   262,  3210,  3033,   286,   262,  2168,   837,\n",
      "          340,   635, 25289,  3294, 16895,   837,   884,   355,  1642,   262,\n",
      "          983,   517, 43486,   329,  2168, 29661,   764, 15684, 11915,   371,\n",
      "         4548,    64,  8835,    73,   280,   290, 26777,  7286, 13704, 13231,\n",
      "        43354,  1111,  4504,   422,  2180, 12784,   837,  1863,   351,   569,\n",
      "        18354,  7496, 17740,  2873,  3437, 33687,  5303, 18024,  6909,   764,\n",
      "          317,  1588,  1074,   286,  8786, 12118,   262,  4226,   764,   383,\n",
      "          983,   705,    82,  4756,  7505,   373, 23568,   416,  1737,   705,\n",
      "           77,   764,   220,   198, 50256,   632,  1138,   351,  3967,  4200,\n",
      "          287,  2869,   837,   290,   373, 15342,   416,  1111,  4960,   290,\n",
      "         8830,  9188,   764,  2293,  2650,   837,   340,  2722, 41496,  2695,\n",
      "          837,  1863,   351,   281,  9902,  8313,   287,  3389,   286,   326,\n",
      "          614,   764,   632,   373,   635, 16573,   656, 15911,   290,   281,\n",
      "         2656,  2008, 11034,  2168,   764, 14444,   284,  1877,  4200,   286,\n",
      "          569, 18354,  7496, 17740,  2873,   837,   569, 18354,  7496, 17740,\n",
      "         6711,   373,   407, 36618,   837,   475,   257,  4336, 11059, 11670,\n",
      "          351,   262,   983,   705,    82,  9902,  8313,   373,  2716,   287,\n",
      "         1946,   764,  6343,    13, 44206,   561,  1441,   284,   262,  8663,\n",
      "          351,   262,  2478,   286,   569, 18354,  7496,  1058, 22134,  9303,\n",
      "          329,   262, 14047,   604,   764,   220,   198, 50256,   796,   796,\n",
      "         3776,  1759,   796,   796,   220,   198, 50256,  1081,   351,  2180,\n",
      "          569, 18354,  8704, 17740,  1830,   837,   569, 18354,  7496, 17740,\n",
      "         6711,   318,   257, 16106,  2597,  2488,    12,    31,  2712,   983,\n",
      "          810,  1938,  1011,  1630,   286,   257,  2422,  4326,   290,  1011,\n",
      "          636,   287, 10566,  1028,  4472,  3386,   764, 18152,   389,  1297,\n",
      "          832,  9048,  1492,  2488,    12,    31,   588, 13043,   351, 15108,\n",
      "         2095, 31725,   837,   351,  3435,  5486, 12387,   832, 21346,  4046,\n",
      "        25037,   290, 12387,   832,   555, 13038,  3711,  2420,   764,   383,\n",
      "         2137, 33226,   832,   257,  2168,   286, 14174, 10566,   837, 11835,\n",
      "        14838,   355,  8739,   326,   460,   307, 12748, 28660,   832,   290,\n",
      "          302, 21542,   355,   484,   389, 14838,   764,   383,  6339,   284,\n",
      "         1123,  1621]), 'labels': tensor([  796,   569, 18354,  7496, 17740,  6711,   796,   220,   198, 50256,\n",
      "         2311,    73, 13090,   645,   569, 18354,  7496,   513,  1058,   791,\n",
      "        47398, 17740,   357,  4960,  1058, 10545,   230,    99,   161,   254,\n",
      "          112,  5641, 44444,  9202, 25084, 24440, 12675, 11839,    18,   837,\n",
      "         6578,   764,   569, 18354,  7496,   286,   262, 30193,   513,  1267,\n",
      "          837,  8811,  6412,   284,   355,   569, 18354,  7496, 17740,  6711,\n",
      "         2354,  2869,   837,   318,   257, 16106,  2597,  2488,    12,    31,\n",
      "         2712,  2008,   983,  4166,   416, 29490,   290,  6343,    13, 44206,\n",
      "          329,   262, 14047, 44685,   764, 28728,   287,  3269,  2813,   287,\n",
      "         2869,   837,   340,   318,   262,  2368,   983,   287,   262,   569,\n",
      "        18354,  7496,  2168,   764, 12645,   278,   262,   976, 21748,   286,\n",
      "        16106,   290,  1103,  2488,    12,    31,   640, 11327,   355,   663,\n",
      "        27677,   837,   262,  1621,  4539, 10730,   284,   262,   717,   983,\n",
      "          290,  5679,   262,   366, 17871,  5321,   366,   837,   257, 23634,\n",
      "         2422,  4326,  7351,   262,  3277,   286,  7096,   544,  1141,   262,\n",
      "         5498,  1898,  6839,  1810,   508,  1620,  3200,  2042,  4560,   290,\n",
      "          389, 46852,  1028,   262, 11773,  4326,   366,  2199,   321,   265,\n",
      "           88, 12552,   366,   764,   220,   198, 50256,   383,   983,  2540,\n",
      "         2478,   287,  3050,   837,  6872,   625,   257,  1588,  6903,   286,\n",
      "          262,   670,  1760,   319,   569, 18354,  7496, 17740,  2873,   764,\n",
      "         2893,   340, 17383,   262,  3210,  3033,   286,   262,  2168,   837,\n",
      "          340,   635, 25289,  3294, 16895,   837,   884,   355,  1642,   262,\n",
      "          983,   517, 43486,   329,  2168, 29661,   764, 15684, 11915,   371,\n",
      "         4548,    64,  8835,    73,   280,   290, 26777,  7286, 13704, 13231,\n",
      "        43354,  1111,  4504,   422,  2180, 12784,   837,  1863,   351,   569,\n",
      "        18354,  7496, 17740,  2873,  3437, 33687,  5303, 18024,  6909,   764,\n",
      "          317,  1588,  1074,   286,  8786, 12118,   262,  4226,   764,   383,\n",
      "          983,   705,    82,  4756,  7505,   373, 23568,   416,  1737,   705,\n",
      "           77,   764,   220,   198, 50256,   632,  1138,   351,  3967,  4200,\n",
      "          287,  2869,   837,   290,   373, 15342,   416,  1111,  4960,   290,\n",
      "         8830,  9188,   764,  2293,  2650,   837,   340,  2722, 41496,  2695,\n",
      "          837,  1863,   351,   281,  9902,  8313,   287,  3389,   286,   326,\n",
      "          614,   764,   632,   373,   635, 16573,   656, 15911,   290,   281,\n",
      "         2656,  2008, 11034,  2168,   764, 14444,   284,  1877,  4200,   286,\n",
      "          569, 18354,  7496, 17740,  2873,   837,   569, 18354,  7496, 17740,\n",
      "         6711,   373,   407, 36618,   837,   475,   257,  4336, 11059, 11670,\n",
      "          351,   262,   983,   705,    82,  9902,  8313,   373,  2716,   287,\n",
      "         1946,   764,  6343,    13, 44206,   561,  1441,   284,   262,  8663,\n",
      "          351,   262,  2478,   286,   569, 18354,  7496,  1058, 22134,  9303,\n",
      "          329,   262, 14047,   604,   764,   220,   198, 50256,   796,   796,\n",
      "         3776,  1759,   796,   796,   220,   198, 50256,  1081,   351,  2180,\n",
      "          569, 18354,  8704, 17740,  1830,   837,   569, 18354,  7496, 17740,\n",
      "         6711,   318,   257, 16106,  2597,  2488,    12,    31,  2712,   983,\n",
      "          810,  1938,  1011,  1630,   286,   257,  2422,  4326,   290,  1011,\n",
      "          636,   287, 10566,  1028,  4472,  3386,   764, 18152,   389,  1297,\n",
      "          832,  9048,  1492,  2488,    12,    31,   588, 13043,   351, 15108,\n",
      "         2095, 31725,   837,   351,  3435,  5486, 12387,   832, 21346,  4046,\n",
      "        25037,   290, 12387,   832,   555, 13038,  3711,  2420,   764,   383,\n",
      "         2137, 33226,   832,   257,  2168,   286, 14174, 10566,   837, 11835,\n",
      "        14838,   355,  8739,   326,   460,   307, 12748, 28660,   832,   290,\n",
      "          302, 21542,   355,   484,   389, 14838,   764,   383,  6339,   284,\n",
      "         1123,  1621])}\n",
      "4718\n"
     ]
    }
   ],
   "source": [
    "# Load the configuration and use it to instantiate the dataset configuration\n",
    "# config = load_config(\"config.yaml\")\n",
    "config = {\n",
    "    \"cache_dir\": \"cache\",\n",
    "    \"seq_len\": 512,\n",
    "}\n",
    "\n",
    "dataset_config = DatasetProviderConfig.from_dict(config)\n",
    "dataset_provider = LMDatasetProvider.from_config(dataset_config)\n",
    "\n",
    "dataset = dataset_provider.get_train_dataset()\n",
    "print(dataset[0])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, PhyAGI exposes a set of **getter functions** that handle provider creation, caching, and concatenation of multiple datasets behind the scenes.\n",
    "\n",
    "`get_dataset` is especially handy when your training or fine‑tuning job involves more than one dataset. You can even oversample or undersample each dataset via the `weight` parameter, as demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[phyagi] [2025-05-19 09:05:29,029] [INFO] [dataset.py:108:get_dataset] Loading datasets...\n",
      "[phyagi] [2025-05-19 09:05:29,032] [INFO] [dataset.py:114:get_dataset] Datasets: [DatasetProviderConfig(cache_dir=PosixPath('/home/gderosa/phyagi-sdk/docs/tutorials/cache'), train_file=PosixPath('/home/gderosa/phyagi-sdk/docs/tutorials/cache/train.npy'), validation_file=PosixPath('/home/gderosa/phyagi-sdk/docs/tutorials/cache/validation.npy'), validation_split=None, tokenizer_file=None, seq_len=512, shift_labels=False, weight=1.5, label='', ignore_token_id=-100, seed=42, random_mask_prob=None), DatasetProviderConfig(cache_dir=PosixPath('/home/gderosa/phyagi-sdk/docs/tutorials/cache'), train_file=PosixPath('/home/gderosa/phyagi-sdk/docs/tutorials/cache/train.npy'), validation_file=PosixPath('/home/gderosa/phyagi-sdk/docs/tutorials/cache/validation.npy'), validation_split=None, tokenizer_file=None, seq_len=512, shift_labels=False, weight=1.5, label='', ignore_token_id=-100, seed=42, random_mask_prob=None)]\n",
      "[phyagi] [2025-05-19 09:05:29,032] [INFO] [dataset.py:115:get_dataset] Global weight multiplier: 1.0\n",
      "{'input_ids': tensor([  796,   569, 18354,  7496, 17740,  6711,   796,   220,   198, 50256,\n",
      "         2311,    73, 13090,   645,   569, 18354,  7496,   513,  1058,   791,\n",
      "        47398, 17740,   357,  4960,  1058, 10545,   230,    99,   161,   254,\n",
      "          112,  5641, 44444,  9202, 25084, 24440, 12675, 11839,    18,   837,\n",
      "         6578,   764,   569, 18354,  7496,   286,   262, 30193,   513,  1267,\n",
      "          837,  8811,  6412,   284,   355,   569, 18354,  7496, 17740,  6711,\n",
      "         2354,  2869,   837,   318,   257, 16106,  2597,  2488,    12,    31,\n",
      "         2712,  2008,   983,  4166,   416, 29490,   290,  6343,    13, 44206,\n",
      "          329,   262, 14047, 44685,   764, 28728,   287,  3269,  2813,   287,\n",
      "         2869,   837,   340,   318,   262,  2368,   983,   287,   262,   569,\n",
      "        18354,  7496,  2168,   764, 12645,   278,   262,   976, 21748,   286,\n",
      "        16106,   290,  1103,  2488,    12,    31,   640, 11327,   355,   663,\n",
      "        27677,   837,   262,  1621,  4539, 10730,   284,   262,   717,   983,\n",
      "          290,  5679,   262,   366, 17871,  5321,   366,   837,   257, 23634,\n",
      "         2422,  4326,  7351,   262,  3277,   286,  7096,   544,  1141,   262,\n",
      "         5498,  1898,  6839,  1810,   508,  1620,  3200,  2042,  4560,   290,\n",
      "          389, 46852,  1028,   262, 11773,  4326,   366,  2199,   321,   265,\n",
      "           88, 12552,   366,   764,   220,   198, 50256,   383,   983,  2540,\n",
      "         2478,   287,  3050,   837,  6872,   625,   257,  1588,  6903,   286,\n",
      "          262,   670,  1760,   319,   569, 18354,  7496, 17740,  2873,   764,\n",
      "         2893,   340, 17383,   262,  3210,  3033,   286,   262,  2168,   837,\n",
      "          340,   635, 25289,  3294, 16895,   837,   884,   355,  1642,   262,\n",
      "          983,   517, 43486,   329,  2168, 29661,   764, 15684, 11915,   371,\n",
      "         4548,    64,  8835,    73,   280,   290, 26777,  7286, 13704, 13231,\n",
      "        43354,  1111,  4504,   422,  2180, 12784,   837,  1863,   351,   569,\n",
      "        18354,  7496, 17740,  2873,  3437, 33687,  5303, 18024,  6909,   764,\n",
      "          317,  1588,  1074,   286,  8786, 12118,   262,  4226,   764,   383,\n",
      "          983,   705,    82,  4756,  7505,   373, 23568,   416,  1737,   705,\n",
      "           77,   764,   220,   198, 50256,   632,  1138,   351,  3967,  4200,\n",
      "          287,  2869,   837,   290,   373, 15342,   416,  1111,  4960,   290,\n",
      "         8830,  9188,   764,  2293,  2650,   837,   340,  2722, 41496,  2695,\n",
      "          837,  1863,   351,   281,  9902,  8313,   287,  3389,   286,   326,\n",
      "          614,   764,   632,   373,   635, 16573,   656, 15911,   290,   281,\n",
      "         2656,  2008, 11034,  2168,   764, 14444,   284,  1877,  4200,   286,\n",
      "          569, 18354,  7496, 17740,  2873,   837,   569, 18354,  7496, 17740,\n",
      "         6711,   373,   407, 36618,   837,   475,   257,  4336, 11059, 11670,\n",
      "          351,   262,   983,   705,    82,  9902,  8313,   373,  2716,   287,\n",
      "         1946,   764,  6343,    13, 44206,   561,  1441,   284,   262,  8663,\n",
      "          351,   262,  2478,   286,   569, 18354,  7496,  1058, 22134,  9303,\n",
      "          329,   262, 14047,   604,   764,   220,   198, 50256,   796,   796,\n",
      "         3776,  1759,   796,   796,   220,   198, 50256,  1081,   351,  2180,\n",
      "          569, 18354,  8704, 17740,  1830,   837,   569, 18354,  7496, 17740,\n",
      "         6711,   318,   257, 16106,  2597,  2488,    12,    31,  2712,   983,\n",
      "          810,  1938,  1011,  1630,   286,   257,  2422,  4326,   290,  1011,\n",
      "          636,   287, 10566,  1028,  4472,  3386,   764, 18152,   389,  1297,\n",
      "          832,  9048,  1492,  2488,    12,    31,   588, 13043,   351, 15108,\n",
      "         2095, 31725,   837,   351,  3435,  5486, 12387,   832, 21346,  4046,\n",
      "        25037,   290, 12387,   832,   555, 13038,  3711,  2420,   764,   383,\n",
      "         2137, 33226,   832,   257,  2168,   286, 14174, 10566,   837, 11835,\n",
      "        14838,   355,  8739,   326,   460,   307, 12748, 28660,   832,   290,\n",
      "          302, 21542,   355,   484,   389, 14838,   764,   383,  6339,   284,\n",
      "         1123,  1621]), 'labels': tensor([  796,   569, 18354,  7496, 17740,  6711,   796,   220,   198, 50256,\n",
      "         2311,    73, 13090,   645,   569, 18354,  7496,   513,  1058,   791,\n",
      "        47398, 17740,   357,  4960,  1058, 10545,   230,    99,   161,   254,\n",
      "          112,  5641, 44444,  9202, 25084, 24440, 12675, 11839,    18,   837,\n",
      "         6578,   764,   569, 18354,  7496,   286,   262, 30193,   513,  1267,\n",
      "          837,  8811,  6412,   284,   355,   569, 18354,  7496, 17740,  6711,\n",
      "         2354,  2869,   837,   318,   257, 16106,  2597,  2488,    12,    31,\n",
      "         2712,  2008,   983,  4166,   416, 29490,   290,  6343,    13, 44206,\n",
      "          329,   262, 14047, 44685,   764, 28728,   287,  3269,  2813,   287,\n",
      "         2869,   837,   340,   318,   262,  2368,   983,   287,   262,   569,\n",
      "        18354,  7496,  2168,   764, 12645,   278,   262,   976, 21748,   286,\n",
      "        16106,   290,  1103,  2488,    12,    31,   640, 11327,   355,   663,\n",
      "        27677,   837,   262,  1621,  4539, 10730,   284,   262,   717,   983,\n",
      "          290,  5679,   262,   366, 17871,  5321,   366,   837,   257, 23634,\n",
      "         2422,  4326,  7351,   262,  3277,   286,  7096,   544,  1141,   262,\n",
      "         5498,  1898,  6839,  1810,   508,  1620,  3200,  2042,  4560,   290,\n",
      "          389, 46852,  1028,   262, 11773,  4326,   366,  2199,   321,   265,\n",
      "           88, 12552,   366,   764,   220,   198, 50256,   383,   983,  2540,\n",
      "         2478,   287,  3050,   837,  6872,   625,   257,  1588,  6903,   286,\n",
      "          262,   670,  1760,   319,   569, 18354,  7496, 17740,  2873,   764,\n",
      "         2893,   340, 17383,   262,  3210,  3033,   286,   262,  2168,   837,\n",
      "          340,   635, 25289,  3294, 16895,   837,   884,   355,  1642,   262,\n",
      "          983,   517, 43486,   329,  2168, 29661,   764, 15684, 11915,   371,\n",
      "         4548,    64,  8835,    73,   280,   290, 26777,  7286, 13704, 13231,\n",
      "        43354,  1111,  4504,   422,  2180, 12784,   837,  1863,   351,   569,\n",
      "        18354,  7496, 17740,  2873,  3437, 33687,  5303, 18024,  6909,   764,\n",
      "          317,  1588,  1074,   286,  8786, 12118,   262,  4226,   764,   383,\n",
      "          983,   705,    82,  4756,  7505,   373, 23568,   416,  1737,   705,\n",
      "           77,   764,   220,   198, 50256,   632,  1138,   351,  3967,  4200,\n",
      "          287,  2869,   837,   290,   373, 15342,   416,  1111,  4960,   290,\n",
      "         8830,  9188,   764,  2293,  2650,   837,   340,  2722, 41496,  2695,\n",
      "          837,  1863,   351,   281,  9902,  8313,   287,  3389,   286,   326,\n",
      "          614,   764,   632,   373,   635, 16573,   656, 15911,   290,   281,\n",
      "         2656,  2008, 11034,  2168,   764, 14444,   284,  1877,  4200,   286,\n",
      "          569, 18354,  7496, 17740,  2873,   837,   569, 18354,  7496, 17740,\n",
      "         6711,   373,   407, 36618,   837,   475,   257,  4336, 11059, 11670,\n",
      "          351,   262,   983,   705,    82,  9902,  8313,   373,  2716,   287,\n",
      "         1946,   764,  6343,    13, 44206,   561,  1441,   284,   262,  8663,\n",
      "          351,   262,  2478,   286,   569, 18354,  7496,  1058, 22134,  9303,\n",
      "          329,   262, 14047,   604,   764,   220,   198, 50256,   796,   796,\n",
      "         3776,  1759,   796,   796,   220,   198, 50256,  1081,   351,  2180,\n",
      "          569, 18354,  8704, 17740,  1830,   837,   569, 18354,  7496, 17740,\n",
      "         6711,   318,   257, 16106,  2597,  2488,    12,    31,  2712,   983,\n",
      "          810,  1938,  1011,  1630,   286,   257,  2422,  4326,   290,  1011,\n",
      "          636,   287, 10566,  1028,  4472,  3386,   764, 18152,   389,  1297,\n",
      "          832,  9048,  1492,  2488,    12,    31,   588, 13043,   351, 15108,\n",
      "         2095, 31725,   837,   351,  3435,  5486, 12387,   832, 21346,  4046,\n",
      "        25037,   290, 12387,   832,   555, 13038,  3711,  2420,   764,   383,\n",
      "         2137, 33226,   832,   257,  2168,   286, 14174, 10566,   837, 11835,\n",
      "        14838,   355,  8739,   326,   460,   307, 12748, 28660,   832,   290,\n",
      "          302, 21542,   355,   484,   389, 14838,   764,   383,  6339,   284,\n",
      "         1123,  1621])}\n",
      "14154\n"
     ]
    }
   ],
   "source": [
    "from phyagi.datasets.registry import get_dataset\n",
    "\n",
    "config = {\n",
    "    \"cache_dir\": \"cache\",\n",
    "    \"seq_len\": 512,\n",
    "    \"weight\": 1.5,\n",
    "}\n",
    "\n",
    "dataset, _ = get_dataset([config, config], dataset_concat=\"random\", dataset_provider=\"lm\")\n",
    "print(dataset[0])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, `get_dataset` instantiates each provider, applies the requested sampling strategy, and concatenates the resulting datasets according to `dataset_concat`.\n",
    "\n",
    "*In the example above, the final length **14154** reflects that each dataset was oversampled by **1.5×** before concatenation.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phyagisdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
