{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture\n",
    "\n",
    "PhyAGI introduces a versatile transformer-based language model known as MixFormer, which is highly adaptable for various tasks. This model adheres to the Hugging Face API standards, making it compatible with different training frameworks such as Hugging Face, PyTorch Lightning, and DeepSpeed.\n",
    "\n",
    "In this guide, we will explore the customization options for MixFormer within the PhyAGI framework and learn how to develop new components to innovate upon the existing architecture.\n",
    "\n",
    "Let's start by creating a basic instance of MixFormer for causal language modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MixFormerSequentialForCausalLM(\n",
      "  (layers): Sequential(\n",
      "    (0): Embedding(\n",
      "      (wte): Embedding(50304, 1024)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): ParallelBlock(\n",
      "      (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=True)\n",
      "        (out_proj): FusedDense(in_features=1024, out_features=1024, bias=True)\n",
      "        (inner_attn): FlashCrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): FusedMLP(\n",
      "        (mlp): FusedMLP(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): ParallelBlock(\n",
      "      (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=True)\n",
      "        (out_proj): FusedDense(in_features=1024, out_features=1024, bias=True)\n",
      "        (inner_attn): FlashCrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): FusedMLP(\n",
      "        (mlp): FusedMLP(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): CausalLMHead(\n",
      "      (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=1024, out_features=50304, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (loss): CausalLMLoss(\n",
      "    (loss_fct): CrossEntropyLoss()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from phyagi.models.mixformer_sequential import MixFormerSequentialConfig, MixFormerSequentialForCausalLM\n",
    "\n",
    "config = MixFormerSequentialConfig(n_layer=2)\n",
    "model = MixFormerSequentialForCausalLM(config)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the architecture\n",
    "\n",
    "MixFormer is composed of the following components:\n",
    "\n",
    "* `embedding`: Embeds input tokens.\n",
    "\n",
    "* `block`: Processes the embedded tokens (typically referred to as a decoder layer).\n",
    "\n",
    "* `mixer`: Mixes the embedded tokens (e.g., attention mechanisms).\n",
    "\n",
    "* `mlp`: Processes the mixer's output.\n",
    "\n",
    "* `norm`: Normalizes the tokens (e.g., `LayerNorm`).\n",
    "\n",
    "* `head`: Processes the block's output and may include components like a linear layer and cross-entropy loss.\n",
    "\n",
    "You can select each component from a variety of available implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: ['default', 'positional']\n",
      "Blocks: ['parallel', 'sequential', 'xyz']\n",
      "Mixers: ['mha', 'conv1d']\n",
      "MLPs: ['glu', 'fused_mlp', 'mlp', 'deep_mlp']\n",
      "Norms: ['torch', 'low_precision', 'rms', 'flash_rms']\n",
      "Heads: ['causal_lm', 'seq_cls']\n",
      "Losses: ['causal_lm', 'seq_cls']\n"
     ]
    }
   ],
   "source": [
    "from phyagi.models.mixformer_sequential.blocks import BLOCKS\n",
    "from phyagi.models.mixformer_sequential.blocks.embeddings import EMBEDDINGS\n",
    "from phyagi.models.mixformer_sequential.blocks.mixers import MIXERS\n",
    "from phyagi.models.mixformer_sequential.blocks.mlps import MLPS\n",
    "from phyagi.models.mixformer_sequential.blocks.norms import NORMS\n",
    "from phyagi.models.mixformer_sequential.blocks.heads import HEADS, LOSSES\n",
    "\n",
    "print(f\"Embeddings: {list(EMBEDDINGS.keys())}\")\n",
    "print(f\"Blocks: {list(BLOCKS.keys())}\")\n",
    "print(f\"Mixers: {list(MIXERS.keys())}\")\n",
    "print(f\"MLPs: {list(MLPS.keys())}\")\n",
    "print(f\"Norms: {list(NORMS.keys())}\")\n",
    "print(f\"Heads: {list(HEADS.keys())}\")\n",
    "print(f\"Losses: {list(LOSSES.keys())}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a custom architecture, you need to define the `block_cls` (string), `mixer` (dict), `mlp` (dict), and `norm` (dict) arguments in the `architecture` dictionary when configuring `MixFormerSequentialConfig`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MixFormerSequentialForCausalLM(\n",
      "  (layers): Sequential(\n",
      "    (0): Embedding(\n",
      "      (wte): Embedding(50304, 1024)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): SequentialBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=True)\n",
      "        (out_proj): FusedDense(in_features=1024, out_features=1024, bias=True)\n",
      "        (inner_attn): FlashCrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (act): NewGELUActivation()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      )\n",
      "      (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): SequentialBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=True)\n",
      "        (out_proj): FusedDense(in_features=1024, out_features=1024, bias=True)\n",
      "        (inner_attn): FlashCrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (act): NewGELUActivation()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      )\n",
      "      (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): CausalLMHead(\n",
      "      (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=1024, out_features=50304, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (loss): CausalLMLoss(\n",
      "    (loss_fct): CrossEntropyLoss()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config = MixFormerSequentialConfig(\n",
    "    n_layer=2,\n",
    "    architecture={\n",
    "        \"block_cls\": \"sequential\",\n",
    "        \"mixer\": {\n",
    "            \"mixer_cls\": \"mha\",\n",
    "        },\n",
    "        \"mlp\": {\n",
    "            \"mlp_cls\": \"mlp\",\n",
    "        },\n",
    "        \"norm\": {\n",
    "            \"norm_cls\": \"torch\",\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "model = MixFormerSequentialForCausalLM(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that embeddings and heads are specified differently. Embeddings are set using `embd_layer` due to legacy reasons, and the head is created directly in the task-specific class (e.g., `MixFormerSequentialForCausalLM`).\n",
    "\n",
    "The guide continues with examples of how to configure MixFormer to replicate various well-known architectures such as CodeGen, LLaMA, and GPT. Each example demonstrates how to adjust the configuration to match the respective architecture's unique properties.\n",
    "\n",
    "### CodeGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MixFormerSequentialForCausalLM(\n",
      "  (layers): Sequential(\n",
      "    (0): Embedding(\n",
      "      (wte): Embedding(50304, 1024)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): ParallelBlock(\n",
      "      (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=True)\n",
      "        (out_proj): FusedDense(in_features=1024, out_features=1024, bias=True)\n",
      "        (inner_attn): FlashCrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (act): NewGELUActivation()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      )\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): ParallelBlock(\n",
      "      (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=True)\n",
      "        (out_proj): FusedDense(in_features=1024, out_features=1024, bias=True)\n",
      "        (inner_attn): FlashCrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (act): NewGELUActivation()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      )\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): CausalLMHead(\n",
      "      (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=1024, out_features=50304, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (loss): CausalLMLoss(\n",
      "    (loss_fct): CrossEntropyLoss()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config = MixFormerSequentialConfig(\n",
    "    n_layer=2,\n",
    "    architecture={\n",
    "        \"block_cls\": \"parallel\",\n",
    "        \"mixer\": {\n",
    "            \"mixer_cls\": \"mha\",\n",
    "            # Additional keyword arguments can be used, please check `phyagi-sdk/models/blocks/mixers/mha.py`\n",
    "        },\n",
    "        \"mlp\": {\n",
    "            \"mlp_cls\": \"mlp\",\n",
    "        },\n",
    "        \"norm\": {\n",
    "            \"norm_cls\": \"torch\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "model = MixFormerSequentialForCausalLM(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA\n",
    "\n",
    "LLaMa architecture uses a trick to initialize the head component as it needs to disable the `bias`. However, there is a fail-safe mechanism where `head_cls` needs to match the class, e.g., if `head_cls` is `causal_lm`, it will only work when initializing with `MixFormerSequentialForCausalLM`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MixFormerSequentialForCausalLM(\n",
      "  (layers): Sequential(\n",
      "    (0): Embedding(\n",
      "      (wte): Embedding(50304, 1024)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): SequentialBlock(\n",
      "      (ln_1): RMSLayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_2): RMSLayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=True)\n",
      "        (out_proj): FusedDense(in_features=1024, out_features=1024, bias=True)\n",
      "        (inner_attn): FlashCrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): GLU(\n",
      "        (act): SiLU()\n",
      "        (fc1): Linear(in_features=1024, out_features=10912, bias=False)\n",
      "        (fc2): Linear(in_features=5456, out_features=1024, bias=False)\n",
      "      )\n",
      "      (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): SequentialBlock(\n",
      "      (ln_1): RMSLayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_2): RMSLayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=True)\n",
      "        (out_proj): FusedDense(in_features=1024, out_features=1024, bias=True)\n",
      "        (inner_attn): FlashCrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): GLU(\n",
      "        (act): SiLU()\n",
      "        (fc1): Linear(in_features=1024, out_features=10912, bias=False)\n",
      "        (fc2): Linear(in_features=5456, out_features=1024, bias=False)\n",
      "      )\n",
      "      (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): CausalLMHead(\n",
      "      (ln): RMSLayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=1024, out_features=50304, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (loss): CausalLMLoss(\n",
      "    (loss_fct): CrossEntropyLoss()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config = MixFormerSequentialConfig(\n",
    "    n_layer=2,\n",
    "    architecture={\n",
    "        \"block_cls\": \"sequential\",\n",
    "        \"mixer\": {\n",
    "            \"mixer_cls\": \"mha\",\n",
    "        },\n",
    "        \"mlp\": {\n",
    "            \"mlp_cls\": \"glu\",\n",
    "            \"act_fn\": \"silu\",\n",
    "            \"n_inner\": 5456, # int(2/3 * 4 * config.n_embd)\n",
    "        },\n",
    "        \"norm\": {\n",
    "            \"norm_cls\": \"rms\",\n",
    "        },\n",
    "        \"head\": {\n",
    "            \"head_cls\": \"causal_lm\",\n",
    "            \"use_bias\": False,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "model = MixFormerSequentialForCausalLM(config)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MixFormerSequentialForCausalLM(\n",
      "  (layers): Sequential(\n",
      "    (0): Embedding(\n",
      "      (wte): Embedding(50304, 1024)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): SequentialBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=True)\n",
      "        (out_proj): FusedDense(in_features=1024, out_features=1024, bias=True)\n",
      "        (inner_attn): FlashCrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): FusedMLP(\n",
      "        (mlp): FusedMLP(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): SequentialBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=True)\n",
      "        (out_proj): FusedDense(in_features=1024, out_features=1024, bias=True)\n",
      "        (inner_attn): FlashCrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): FusedMLP(\n",
      "        (mlp): FusedMLP(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): CausalLMHead(\n",
      "      (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=1024, out_features=50304, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (loss): CausalLMLoss(\n",
      "    (loss_fct): CrossEntropyLoss()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config = MixFormerSequentialConfig(\n",
    "    n_layer=2,\n",
    "    architecture={\n",
    "        \"block_cls\": \"sequential\",\n",
    "        \"mixer\": {\n",
    "            \"mixer_cls\": \"mha\",\n",
    "            \"window_size\": (128, 128)\n",
    "        },\n",
    "        \"mlp\": {\n",
    "            \"mlp_cls\": \"fused_mlp\",\n",
    "            \"n_inner\": 4096\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "model = MixFormerSequentialForCausalLM(config)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heterogeneous architectures\n",
    "\n",
    "MixFormer also supports heterogeneous architectures that employ different types of blocks at different layers. The following configuration illustrates this capability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[phyagi] [2025-05-19 10:30:59,695] [WARNING] [__init__.py:34:get_block] `config.n_layer` does not match number of blocks in `block_config`. Overriding 1 with 2.\n",
      "MixFormerSequentialForCausalLM(\n",
      "  (layers): Sequential(\n",
      "    (0): Embedding(\n",
      "      (wte): Embedding(50304, 1024)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): SequentialBlock(\n",
      "      (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=True)\n",
      "        (out_proj): FusedDense(in_features=1024, out_features=1024, bias=True)\n",
      "        (inner_attn): FlashCrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): FusedMLP(\n",
      "        (mlp): FusedMLP(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): ParallelBlock(\n",
      "      (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (mixer): MHA(\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (Wqkv): FusedDense(in_features=1024, out_features=3072, bias=True)\n",
      "        (out_proj): FusedDense(in_features=1024, out_features=1024, bias=True)\n",
      "        (inner_attn): FlashCrossAttention(\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (mlp): GLU(\n",
      "        (act): NewGELUActivation()\n",
      "        (fc1): Linear(in_features=1024, out_features=8192, bias=False)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      )\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): CausalLMHead(\n",
      "      (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear): Linear(in_features=1024, out_features=50304, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (loss): CausalLMLoss(\n",
      "    (loss_fct): CrossEntropyLoss()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config = MixFormerSequentialConfig(\n",
    "    n_layer=1,\n",
    "    architecture=[\n",
    "        {\n",
    "            \"block_cls\": \"sequential\",\n",
    "            \"mixer\": {\n",
    "                \"mixer_cls\": \"mha\",\n",
    "                \"window_size\": 128,\n",
    "            },\n",
    "            \"mlp\": {\n",
    "                \"mlp_cls\": \"fused_mlp\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"block_cls\": \"parallel\",\n",
    "            \"mixer\": {\n",
    "                \"mixer_cls\": \"mha\",\n",
    "            },\n",
    "            \"mlp\": {\n",
    "                \"mlp_cls\": \"glu\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = MixFormerSequentialForCausalLM(config)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will automatically adjust the `config.n_layer` to accommodate the actual number of layers specified in the configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New architectures\n",
    "\n",
    "MixFormer is designed with extensibility in mind, allowing for the straightforward addition of new blocks, mixers, MLPs, and other components.\n",
    "\n",
    "### Mixer\n",
    "\n",
    "To create a new mixer, adhere to the following guidelines:  \n",
    "  \n",
    "- The class `__init__` method must accept a `config` argument. \n",
    " \n",
    "- The `forward` method must return a tensor with the same dimensions as the input tensor.  \n",
    "  \n",
    "Here's an example of implementing a `Conv1dMixer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PretrainedConfig\n",
    "\n",
    "\n",
    "class Conv1dMixer(torch.nn.Module):\n",
    "    def __init__(self, config: PretrainedConfig, kernel_size: int = 3, layer_idx: int = None) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = torch.nn.Conv1d(\n",
    "            in_channels=config.n_embd, \n",
    "            out_channels=config.n_embd, \n",
    "            kernel_size=kernel_size, \n",
    "            padding=kernel_size-1,\n",
    "            groups=config.n_embd\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.FloatTensor, **kwargs) -> torch.FloatTensor:\n",
    "        out = self.conv(x.transpose(1, 2)).transpose(1, 2)\n",
    "        return out[:, :x.shape[1]], None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the new mixer accessible within `phyagi`, you can:\n",
    "\n",
    "1. Submit a pull request to include the mixer in the `phyagi-sdk/models/blocks/mixers/__init__.py` file, making it available to all users.\n",
    "\n",
    "2. Register it dynamically by adding a new key to the `MIXERS` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phyagi.models.mixformer_sequential.blocks.mixers import MIXERS\n",
    "\n",
    "MIXERS[\"conv1d\"] = Conv1dMixer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After registration, you can confirm its availability using `layers` attribute and instantiate it via the `mixer_cls` argument in the `architecture` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mha', 'conv1d']\n",
      "tensor([[[-0.3493, -0.5180,  0.3647,  ...,  0.1448,  0.1304, -0.9054],\n",
      "         [-0.5930, -0.2169,  0.6949,  ...,  0.1118,  0.0732, -0.5457],\n",
      "         [-0.6067,  0.0864,  0.8189,  ...,  0.4726, -0.0754, -0.3408],\n",
      "         ...,\n",
      "         [-0.4264,  0.4139,  0.7655,  ...,  0.2481, -0.4173, -0.3329],\n",
      "         [-0.4264,  0.4139,  0.7655,  ...,  0.2481, -0.4173, -0.3329],\n",
      "         [-0.4264,  0.4139,  0.7655,  ...,  0.2481, -0.4173, -0.3329]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from phyagi.models.mixformer_sequential import MixFormerSequentialConfig, MixFormerSequentialForCausalLM\n",
    "from phyagi.models.mixformer_sequential.blocks.mixers import MIXERS\n",
    "\n",
    "print(list(MIXERS.keys()))\n",
    "\n",
    "config = MixFormerSequentialConfig(\n",
    "    n_layer=2,\n",
    "    architecture={\n",
    "        \"block_cls\": \"sequential\",\n",
    "        \"mixer\": {\n",
    "            \"mixer_cls\": \"conv1d\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "model = MixFormerSequentialForCausalLM(config)\n",
    "print(model(torch.zeros((1, 1024), dtype=torch.long)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra keys in the `mixer` dictionary serve as keyword arguments for the mixer class, allowing for customization, such as changing the `kernel_size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MixFormerSequentialConfig(\n",
    "    n_layer=2,\n",
    "    architecture={\n",
    "        \"block_cls\": \"sequential\",\n",
    "        \"mixer\": {\n",
    "            \"mixer_cls\": \"conv1d\",\n",
    "            \"kernel_size\": 11\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "model = MixFormerSequentialForCausalLM(config)\n",
    "assert model.layers[1].attn.conv.kernel_size == (11,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "\n",
    "The same principles apply when creating a new MLP:\n",
    "\n",
    "* The class `__init__` method must accept a `config` argument.\n",
    "\n",
    "* The `forward` method must return a tensor with the same dimensions as the input tensor.\n",
    "\n",
    "Here's an example of a `DeepMLP`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.activations import ACT2FN\n",
    "\n",
    "\n",
    "class DeepMLP(torch.nn.Module):\n",
    "    def __init__(self, config: PretrainedConfig, n_layer: int = 3, n_inner: int = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_inner = n_inner or config.n_inner or 4*config.n_embd\n",
    "        self.act = ACT2FN[config.activation_function]\n",
    "\n",
    "        layers = [torch.nn.Linear(config.n_embd, self.n_inner)]\n",
    "        layers += [torch.nn.Linear(self.n_inner, self.n_inner) for _ in range(n_layer - 2)]\n",
    "        layers += [torch.nn.Linear(self.n_inner, config.n_embd)]\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x: torch.FloatTensor, **kwargs) -> torch.FloatTensor:\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.act(layer(x))\n",
    "        \n",
    "        return self.layers[-1](x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glu', 'fused_mlp', 'mlp', 'deep_mlp']\n"
     ]
    }
   ],
   "source": [
    "from phyagi.models.mixformer_sequential.blocks.mlps import MLPS\n",
    "\n",
    "MLPS[\"deep_mlp\"] = DeepMLP\n",
    "print(list(MLPS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1301,  0.2461,  0.1236,  ..., -0.1361, -0.0135, -0.0081],\n",
      "         [-0.2107, -0.3234,  0.3190,  ...,  0.2561, -0.5194,  0.2247],\n",
      "         [ 0.3618, -0.3681,  0.2278,  ...,  0.1237, -0.3493, -0.2407],\n",
      "         ...,\n",
      "         [ 0.1465, -0.7555,  0.0601,  ..., -0.0617, -0.2871, -0.1053],\n",
      "         [ 0.1465, -0.7555,  0.0601,  ..., -0.0617, -0.2871, -0.1053],\n",
      "         [ 0.1465, -0.7555,  0.0601,  ..., -0.0617, -0.2871, -0.1053]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "config = MixFormerSequentialConfig(\n",
    "    n_layer=2,\n",
    "    architecture={\n",
    "        \"block_cls\": \"sequential\",\n",
    "        \"mixer\": {\n",
    "            \"mixer_cls\": \"conv1d\"\n",
    "        },\n",
    "        \"mlp\": {\n",
    "            \"mlp_cls\": \"deep_mlp\",\n",
    "            \"n_layer\": 4,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "model = MixFormerSequentialForCausalLM(config)\n",
    "print(model(torch.zeros((1, 1024), dtype=torch.long)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block\n",
    "\n",
    "To create a new block component:\n",
    "\n",
    "* The class `__init__` method must include a `config` argument.\n",
    "\n",
    "* Optionally, it may include a `block_idx` argument.\n",
    "\n",
    "* Preferably, it should accept `mlp` and `mixer` arguments.\n",
    "\n",
    "* The `forward` method must output a tensor maintaining the input's dimensions.\n",
    "\n",
    "Example of a custom block implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Tuple, Optional, Union\n",
    "from phyagi.models.mixformer_sequential.blocks.mixers import get_mixer\n",
    "from phyagi.models.mixformer_sequential.blocks.mlps import get_mlp\n",
    "\n",
    "\n",
    "class BlockXyz(torch.nn.Module):\n",
    "    def __init__(self, config: PretrainedConfig, mixer: Dict[str, Any], mlp: Dict[str, Any], block_idx: int = None, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mixer = get_mixer(config, mixer_config=mixer)\n",
    "        self.mlp = get_mlp(config, mlp_config=mlp)\n",
    "        self.block_idx = block_idx\n",
    "    \n",
    "    def forward(self, x: Union[torch.FloatTensor, Tuple[torch.FloatTensor]], **kwargs) -> Tuple[\n",
    "        torch.FloatTensor,\n",
    "        Optional[torch.FloatTensor],\n",
    "        Optional[torch.FloatTensor],\n",
    "        Optional[Tuple[torch.FloatTensor, torch.FloatTensor]],\n",
    "    ]:\n",
    "        x = self.mixer(x)[0]\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        return (x, None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parallel', 'sequential', 'xyz']\n"
     ]
    }
   ],
   "source": [
    "from phyagi.models.mixformer_sequential.blocks import BLOCKS\n",
    "\n",
    "BLOCKS[\"xyz\"] = BlockXyz\n",
    "print(list(BLOCKS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3602, -0.9025,  0.0893,  ..., -0.3397, -0.6635,  0.4196],\n",
      "         [-0.3642, -0.9088,  0.0790,  ..., -0.3380, -0.6663,  0.4473],\n",
      "         [-0.3450, -0.9090,  0.1206,  ..., -0.3576, -0.6528,  0.4615],\n",
      "         ...,\n",
      "         [-0.3618, -0.9667,  0.1016,  ..., -0.2696, -0.7573,  0.4785],\n",
      "         [-0.3618, -0.9667,  0.1016,  ..., -0.2696, -0.7573,  0.4785],\n",
      "         [-0.3618, -0.9667,  0.1016,  ..., -0.2696, -0.7573,  0.4785]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "config = MixFormerSequentialConfig(\n",
    "    n_layer=2,\n",
    "    architecture={\n",
    "        \"block_cls\": \"xyz\",\n",
    "        \"mixer\": {\n",
    "            \"mixer_cls\": \"conv1d\",\n",
    "            \"kernel_size\": 11\n",
    "        },\n",
    "        \"mlp\": {\n",
    "            \"mlp_cls\": \"deep_mlp\",\n",
    "            \"n_layer\": 4,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "model = MixFormerSequentialForCausalLM(config)\n",
    "print(model(torch.zeros((1, 1024), dtype=torch.long)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same customization approach can be applied to other components of MixFormer, such as embeddings, heads, and losses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phyagisdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
