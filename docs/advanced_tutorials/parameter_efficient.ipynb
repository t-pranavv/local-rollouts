{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter-Efficient Techniques (PEFT)\n",
    "\n",
    "PEFT lets you adapt a large model to a new task by training only a small set of *adapter* parameters instead of the full model. You save GPU memory, disk space, and—most importantly—time, while often matching the performance of full fine‑tuning.\n",
    "\n",
    "* **Cost‑effective:** only a few million parameters are updated instead of\n",
    "  billions.\n",
    "* **Faster iteration:** shorter training cycles enable rapid experimentation.\n",
    "* **Modularity:** you can keep one base checkpoint and swap in adapters for\n",
    "  different tasks or domains.\n",
    "* **Easy deployment:** the frozen base model stays intact; share or combine\n",
    "  lightweight adapters as needed.\n",
    "\n",
    "In this notebook you will learn how to:\n",
    "\n",
    "1. Load a pre‑trained model.\n",
    "2. Attach a [LoRA](https://arxiv.org/abs/2106.09685) adapter.\n",
    "3. Train the adapter on your dataset.\n",
    "4. Save and reuse the adapter without touching the base weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589,824 || all params: 125,029,632 || trainable%: 0.4717\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using one of PhyAGI's composable models such as `MixFormerSequential`, you need to tell PEFT **which internal modules should receive LoRA layers**.  The snippet below shows a sensible default for decoder‑style transformers; feel free to adjust the `target_modules` list if your architecture uses different block names.\n",
    "\n",
    "**Tip:** Use `model.named_modules()` and look for large `Linear` or `projection` layers to decide which modules carry most of the trainable parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,310,720 || all params: 356,269,184 || trainable%: 0.3679\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from phyagi.models.mixformer_sequential.modeling_mixformer_sequential import MixFormerSequentialConfig, MixFormerSequentialForCausalLM\n",
    "\n",
    "config = MixFormerSequentialConfig()\n",
    "model = MixFormerSequentialForCausalLM(config)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"Wqkv\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the adapter is injected, you can fit it with any training loop:\n",
    "\n",
    "* `HfTrainer`: quick experiments on a single GPU.\n",
    "* `PlTrainer`: clean multi‑GPU / TPU scaling.\n",
    "* `DsTrainer`: when you need to squeeze every last bit of memory.\n",
    "\n",
    "Because only the LoRA weights are trainable, **use a higher learning rate** (e.g. `2 × 10⁻⁴`) and shorten the schedule; adapters converge quickly.\n",
    "\n",
    "**Note**: The base model remains frozen on disk. After training, only ≈ 5 MB of LoRA weights are saved. Distribute or chain them without redistributing the original checkpoint.\n",
    "\n",
    "For a deep dive into advanced adapter types (IA‑3, Prefix‑Tuning, Prompt Tuning, etc.) see the official [PEFT documentation](https://huggingface.co/docs/peft)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phyagisdk310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
